From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Add custom CC_RISCV32 calling convention and improved call
 support

The TableGen-based calling convention definitions are inflexible, while
writing a function to implement the calling convention is very
straight-forward, and allows difficult cases to be handled more easily. With
this patch adds support for:
* Passing large scalars according to the RV32I calling convention
* Byval arguments
* Passing values on the stack when the argument registers are exhausted

It also documents the ABI lowering that a language frontend is expected to
perform. I would like to drastically simplify these requirements over time,
but this will require further discussion within the LLVM community.
---
 lib/Target/RISCV/RISCVCallingConv.td   |  12 +-
 lib/Target/RISCV/RISCVISelLowering.cpp | 277 +++++++++++++++++-
 lib/Target/RISCV/RISCVISelLowering.h   |   4 +
 lib/Target/RISCV/RISCVInstrInfo.td     |   5 +-
 test/CodeGen/RISCV/byval.ll            |  27 ++
 test/CodeGen/RISCV/calling-conv.ll     | 513 +++++++++++++++++++++++++++++++++
 test/CodeGen/RISCV/calls.ll            |  27 ++
 test/CodeGen/RISCV/fp128.ll            |  40 +++
 8 files changed, 879 insertions(+), 26 deletions(-)
 create mode 100644 test/CodeGen/RISCV/byval.ll
 create mode 100644 test/CodeGen/RISCV/calling-conv.ll
 create mode 100644 test/CodeGen/RISCV/fp128.ll

diff --git a/lib/Target/RISCV/RISCVCallingConv.td b/lib/Target/RISCV/RISCVCallingConv.td
index 05a515e7b76..51a4470f5ca 100644
--- a/lib/Target/RISCV/RISCVCallingConv.td
+++ b/lib/Target/RISCV/RISCVCallingConv.td
@@ -15,17 +15,7 @@
 def RetCC_RISCV32 : CallingConv<[CCIfType<[i32], CCAssignToReg<[X10_32, X11_32]>>]>;
 
 // RISCV 32-bit C Calling convention.
-def CC_RISCV32 : CallingConv<[
-  // Promote i8/i16 args to i32
-  CCIfType<[ i8, i16 ], CCPromoteToType<i32>>,
-
-  // All arguments get passed in integer registers if there is space.
-  CCIfType<[i32], CCAssignToReg<[ X10_32, X11_32, X12_32, X13_32, X14_32,
-                                  X15_32, X16_32, X17_32 ]>>,
-
-  // Could be assigned to the stack in 8-byte aligned units, but unsupported
-  CCAssignToStack<8, 8>
-]>;
+def CC_RISCV32 : CustomCallingConv;
 
 def CSR : CalleeSavedRegs<(add X1_32, X3_32, X4_32, X8_32, X9_32,
                           (sequence "X%u_32", 18, 27))>;
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index 45885da604f..353c287c8be 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -284,6 +284,151 @@ RISCVTargetLowering::EmitInstrWithCustomInserter(MachineInstr &MI,
 }
 
 // Calling Convention Implementation.
+// The expectations for frontend ABI lowering vary from target to target.
+// Ideally, an LLVM frontend would be able to avoid worrying about many ABI
+// details, but this is a longer term goal. For now, we simply try to keep the
+// role of the frontend as simple and well-defined as possible. The rules can
+// be summarised as:
+// * Never split up large scalar arguments. We handle them here.
+// * If a hardfloat calling convention is being used, and the struct may be
+// passed in a pair of registers (fp+fp, int+fp), and both registers are
+// available, then pass as two separate arguments. If either the GPRs or FPRs
+// are exhausted, then pass according to the rule below.
+// * If a struct could never be passed in registers or directly in a stack
+// slot (as it is larger than 2x xlen and the floating point rules don't
+// apply), then pass it using a pointer with the byval attribute
+// * If a struct is less than 2x xlen, then coerce to either a two-element
+// word-sized array or a 2x xlen scalar (depending on alignment).
+// * The frontend can determine whether a struct is returned by reference or
+// not based on its size and fields. If it will be returned by reference, the
+// frontend must modify the prototype so a pointer with the sret annotation is
+// passed as the first argument. This is not necessary for large scalar
+// returns.
+// * Struct return values and varargs should be coerced to structs containing
+// register-size fields in the same situations they would be for fixed
+// arguments.
+
+static const MCPhysReg ArgGPRs[] = {
+  RISCV::X10_32, RISCV::X11_32, RISCV::X12_32, RISCV::X13_32,
+  RISCV::X14_32, RISCV::X15_32, RISCV::X16_32, RISCV::X17_32
+};
+
+// Pass a 2xlen argument that has been split in to two xlen values through
+// registers or the stack as necessary.
+static bool CC_RISCV32Assign2XLEN(CCState &State, CCValAssign VA1,
+                                  ISD::ArgFlagsTy ArgFlags1, unsigned ValNo2,
+                                  MVT ValVT2, MVT LocVT2,
+                                  ISD::ArgFlagsTy ArgFlags2) {
+  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {
+    // At least one half can be passed via register
+    State.addLoc(CCValAssign::getReg(VA1.getValNo(), VA1.getValVT(), Reg,
+                                     VA1.getLocVT(), CCValAssign::Full));
+  } else {
+    // Both halves must be passed on the stack, with proper alignment
+    unsigned StackAlign = std::max(4u, ArgFlags1.getOrigAlign());
+    State.addLoc(CCValAssign::getMem(VA1.getValNo(), VA1.getValVT(),
+                                     State.AllocateStack(4, StackAlign),
+                                     VA1.getLocVT(), CCValAssign::Full));
+    State.addLoc(CCValAssign::getMem(ValNo2, ValVT2, State.AllocateStack(4, 4),
+                                     LocVT2, CCValAssign::Full));
+    return false;
+  }
+
+  if (unsigned Reg = State.AllocateReg(ArgGPRs)) {
+    // The second half can also be passed via register
+    State.addLoc(
+        CCValAssign::getReg(ValNo2, ValVT2, Reg, LocVT2, CCValAssign::Full));
+  } else {
+    // The second half is passed via the stack, without additional alignment
+    State.addLoc(CCValAssign::getMem(ValNo2, ValVT2, State.AllocateStack(4, 4),
+                                     LocVT2, CCValAssign::Full));
+  }
+
+  return false;
+}
+
+static bool CC_RISCV32(unsigned ValNo, MVT ValVT, MVT LocVT,
+                       CCValAssign::LocInfo LocInfo, ISD::ArgFlagsTy ArgFlags,
+                       CCState &State) {
+  // Promote i8 and i16
+  if (LocVT == MVT::i8 || LocVT == MVT::i16) {
+    LocVT = MVT::i32;
+    if (ArgFlags.isSExt())
+      LocInfo = CCValAssign::SExt;
+    else if (ArgFlags.isZExt())
+      LocInfo = CCValAssign::ZExt;
+    else
+      LocInfo = CCValAssign::AExt;
+  }
+
+  SmallVectorImpl<CCValAssign> &PendingMembers = State.getPendingLocs();
+  SmallVectorImpl<ISD::ArgFlagsTy> &PendingArgFlags =
+      State.getPendingArgFlags();
+
+  assert(PendingMembers.size() == PendingArgFlags.size());
+
+  // Split arguments might be passed indirectly, so keep track of the pending
+  // values.
+  if (ArgFlags.isSplit() || !PendingMembers.empty()) {
+    LocVT = MVT::i32;
+    LocInfo = CCValAssign::Indirect;
+    PendingMembers.push_back(
+        CCValAssign::getPending(ValNo, ValVT, LocVT, LocInfo));
+    PendingArgFlags.push_back(ArgFlags);
+    if (!ArgFlags.isSplitEnd()) {
+      return false;
+    }
+  }
+
+  // If the split argument only had two elements, it should be passed directly
+  // in registers or on the stack
+  if (ArgFlags.isSplitEnd() && PendingMembers.size() <= 2) {
+    assert(PendingMembers.size() == 2);
+    // Apply the normal calling convention rules to the first half of the
+    // split argument
+    CCValAssign VA = PendingMembers[0];
+    ISD::ArgFlagsTy AF = PendingArgFlags[0];
+    PendingMembers.clear();
+    PendingArgFlags.clear();
+    return CC_RISCV32Assign2XLEN(State, VA, AF, ValNo, ValVT, LocVT, ArgFlags);
+  }
+
+  // Allocate to a register if possible, or else a stack slot
+  unsigned Reg = State.AllocateReg(ArgGPRs);
+  unsigned StackOffset = Reg ? 0 : State.AllocateStack(4, 4);
+
+  // If we reach this point and PendingMembers is non-empty, we must be at the
+  // end of a split argument that must be passed indirect
+  if (!PendingMembers.empty()) {
+    assert(ArgFlags.isSplitEnd());
+    assert(PendingMembers.size() > 2);
+
+    for (auto &It : PendingMembers) {
+      if (Reg)
+        It.convertToReg(Reg);
+      else
+        It.convertToMem(StackOffset);
+      State.addLoc(It);
+    }
+    PendingMembers.clear();
+    PendingArgFlags.clear();
+    return false;
+  }
+
+  assert(LocVT.getSizeInBits() == 32 &&
+         "Programming error: should have a 32-bit sized type at this point");
+  // TODO: do I need to bit-convert f32?
+  assert(LocVT == MVT::i32 && "Expected and i32 at this stage");
+
+  if (Reg) {
+    State.addLoc(CCValAssign::getReg(ValNo, ValVT, Reg, LocVT, LocInfo));
+  } else {
+    State.addLoc(
+        CCValAssign::getMem(ValNo, ValVT, StackOffset, LocVT, LocInfo));
+  }
+  return false;
+}
+
 #include "RISCVGenCallingConv.inc"
 
 // Transform physical registers into virtual registers.
@@ -301,7 +446,9 @@ SDValue RISCVTargetLowering::LowerFormalArguments(
   }
 
   MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo &MFI = MF.getFrameInfo();
   MachineRegisterInfo &RegInfo = MF.getRegInfo();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
 
   if (IsVarArg)
     report_fatal_error("VarArg not supported");
@@ -311,27 +458,59 @@ SDValue RISCVTargetLowering::LowerFormalArguments(
   CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getContext());
   CCInfo.AnalyzeFormalArguments(Ins, CC_RISCV32);
 
-  for (auto &VA : ArgLocs) {
+  for (unsigned I = 0, E = ArgLocs.size(); I != E; ++I) {
+    CCValAssign &VA = ArgLocs[I];
+    EVT LocVT = VA.getLocVT();
+    SDValue ArgValue;
     if (VA.isRegLoc()) {
       // Arguments passed in registers.
-      EVT RegVT = VA.getLocVT();
-      switch (RegVT.getSimpleVT().SimpleTy) {
+      switch (LocVT.getSimpleVT().SimpleTy) {
       case MVT::i32: {
         const unsigned VReg =
             RegInfo.createVirtualRegister(&RISCV::GPRRegClass);
         RegInfo.addLiveIn(VA.getLocReg(), VReg);
-        SDValue ArgIn = DAG.getCopyFromReg(Chain, DL, VReg, RegVT);
-
-        InVals.push_back(ArgIn);
+        ArgValue = DAG.getCopyFromReg(Chain, DL, VReg, LocVT);
         break;
       }
       default:
         DEBUG(dbgs() << "LowerFormalArguments Unhandled argument type: "
-                     << RegVT.getEVTString() << "\n");
+                     << LocVT.getEVTString() << "\n");
         report_fatal_error("unhandled argument type");
       }
     } else {
-      report_fatal_error("Defined with too many args");
+      assert(VA.isMemLoc() && "Argument not register or memory");
+
+      // Create the frame index object for this incoming parameter.
+      int FI = MFI.CreateFixedObject(LocVT.getSizeInBits() / 8,
+                                     VA.getLocMemOffset(), true);
+
+      // Create the SelectionDAG nodes corresponding to a load
+      // from this parameter
+      SDValue FIN = DAG.getFrameIndex(FI, PtrVT);
+      ArgValue = DAG.getLoad(VA.getLocVT(), DL, Chain, FIN,
+                             MachinePointerInfo::getFixedStack(MF, FI));
+    }
+    // Convert the value of the argument register into the value that's
+    // being passed.
+    if (VA.getLocInfo() == CCValAssign::Indirect) {
+      InVals.push_back(DAG.getLoad(VA.getValVT(), DL, Chain, ArgValue,
+                                   MachinePointerInfo()));
+      // If the original argument was split and passed by reference (e.g. i128
+      // on RV32), we need to load all parts of it here (using the same
+      // address).
+      unsigned ArgIndex = Ins[I].OrigArgIndex;
+      assert(Ins[I].PartOffset == 0);
+      while (I + 1 != E && Ins[I + 1].OrigArgIndex == ArgIndex) {
+        CCValAssign &PartVA = ArgLocs[I + 1];
+        unsigned PartOffset = Ins[I + 1].PartOffset;
+        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, ArgValue,
+                                      DAG.getIntPtrConstant(PartOffset, DL));
+        InVals.push_back(DAG.getLoad(PartVA.getValVT(), DL, Chain, Address,
+                                     MachinePointerInfo()));
+        ++I;
+      }
+    } else {
+      InVals.push_back(ArgValue);
     }
   }
   return Chain;
@@ -367,39 +546,101 @@ SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
   // Get a count of how many bytes are to be pushed on the stack.
   unsigned NumBytes = ArgCCInfo.getNextStackOffset();
 
-  for (auto &Arg : Outs) {
-    if (!Arg.Flags.isByVal())
+  // Create local copies for byval args
+  SmallVector<SDValue, 8> ByValArgs;
+  for (unsigned I = 0, E = Outs.size(); I != E; ++I) {
+    ISD::ArgFlagsTy Flags = Outs[I].Flags;
+    if (!Flags.isByVal())
       continue;
-    report_fatal_error("Passing arguments byval not yet implemented");
+
+    SDValue Arg = OutVals[I];
+    unsigned Size = Flags.getByValSize();
+    unsigned Align = Flags.getByValAlign();
+
+    int FI = MF.getFrameInfo().CreateStackObject(Size, Align, false);
+    SDValue FIPtr = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+    // TODO: will need to change for i64
+    SDValue SizeNode = DAG.getConstant(Size, DL, MVT::i32);
+
+    Chain = DAG.getMemcpy(Chain, DL, FIPtr, Arg, SizeNode, Align,
+                          /*IsVolatile=*/false,
+                          /*AlwaysInline=*/false,
+                          /*isTailCall=*/false, MachinePointerInfo(),
+                          MachinePointerInfo());
+    ByValArgs.push_back(FIPtr);
   }
 
   Chain = DAG.getCALLSEQ_START(Chain, NumBytes, 0, CLI.DL);
 
   // Copy argument values to their designated locations.
   SmallVector<std::pair<unsigned, SDValue>, 8> RegsToPass;
+  SmallVector<SDValue, 8> MemOpChains;
   SDValue StackPtr;
-  for (unsigned I = 0, E = ArgLocs.size(); I != E; ++I) {
+  for (unsigned I = 0, J = 0, E = ArgLocs.size(); I != E; ++I) {
     CCValAssign &VA = ArgLocs[I];
     SDValue ArgValue = OutVals[I];
+    ISD::ArgFlagsTy Flags = Outs[I].Flags;
 
     // Promote the value if needed.
-    // For now, only handle fully promoted arguments.
+    // For now, only handle fully promoted and indirect arguments.
     switch (VA.getLocInfo()) {
     case CCValAssign::Full:
       break;
+    case CCValAssign::Indirect: {
+      // Store the argument in a stack slot and pass its address.
+      SDValue SpillSlot = DAG.CreateStackTemporary(Outs[I].ArgVT);
+      int FI = cast<FrameIndexSDNode>(SpillSlot)->getIndex();
+      MemOpChains.push_back(
+          DAG.getStore(Chain, DL, ArgValue, SpillSlot,
+                       MachinePointerInfo::getFixedStack(MF, FI)));
+      // If the original argument was split (e.g. i128), we need
+      // to store all parts of it here (and pass just one address).
+      unsigned ArgIndex = Outs[I].OrigArgIndex;
+      assert(Outs[I].PartOffset == 0);
+      while (I + 1 != E && Outs[I + 1].OrigArgIndex == ArgIndex) {
+        SDValue PartValue = OutVals[I + 1];
+        unsigned PartOffset = Outs[I + 1].PartOffset;
+        SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, SpillSlot,
+                                      DAG.getIntPtrConstant(PartOffset, DL));
+        MemOpChains.push_back(
+            DAG.getStore(Chain, DL, PartValue, Address,
+                         MachinePointerInfo::getFixedStack(MF, FI)));
+        ++I;
+      }
+      ArgValue = SpillSlot;
+      break;
+    }
     default:
       llvm_unreachable("Unknown loc info!");
     }
 
+    // Use local copy if it is a byval arg.
+    if (Flags.isByVal())
+      ArgValue = ByValArgs[J++];
+
     if (VA.isRegLoc()) {
       // Queue up the argument copies and emit them at the end.
       RegsToPass.push_back(std::make_pair(VA.getLocReg(), ArgValue));
     } else {
       assert(VA.isMemLoc() && "Argument not register or memory");
-      report_fatal_error("Passing arguments via the stack not yet implemented");
+
+      // Work out the address of the stack slot.
+      if (!StackPtr.getNode())
+        StackPtr = DAG.getCopyFromReg(Chain, DL, RISCV::X2_32, PtrVT);
+      SDValue Address =
+          DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,
+                      DAG.getIntPtrConstant(VA.getLocMemOffset(), DL));
+
+      // Emit the store.
+      MemOpChains.push_back(
+          DAG.getStore(Chain, DL, ArgValue, Address, MachinePointerInfo()));
     }
   }
 
+  // Join the stores, which are independent of one another.
+  if (!MemOpChains.empty())
+    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOpChains);
+
   SDValue Glue;
 
   // Build a sequence of copy-to-reg nodes, chained and glued together.
@@ -465,6 +706,14 @@ SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
   return Chain;
 }
 
+bool RISCVTargetLowering::CanLowerReturn(
+    CallingConv::ID CallConv, MachineFunction &MF, bool IsVarArg,
+    const SmallVectorImpl<ISD::OutputArg> &Outs, LLVMContext &Context) const {
+  SmallVector<CCValAssign, 16> RVLocs;
+  CCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);
+  return CCInfo.CheckReturn(Outs, RetCC_RISCV32);
+}
+
 SDValue
 RISCVTargetLowering::LowerReturn(SDValue Chain, CallingConv::ID CallConv,
                                  bool IsVarArg,
diff --git a/lib/Target/RISCV/RISCVISelLowering.h b/lib/Target/RISCV/RISCVISelLowering.h
index 77d5f16ee7c..5e3fb118df6 100644
--- a/lib/Target/RISCV/RISCVISelLowering.h
+++ b/lib/Target/RISCV/RISCVISelLowering.h
@@ -54,6 +54,10 @@ private:
                                const SmallVectorImpl<ISD::InputArg> &Ins,
                                const SDLoc &DL, SelectionDAG &DAG,
                                SmallVectorImpl<SDValue> &InVals) const override;
+  bool CanLowerReturn(CallingConv::ID CallConv, MachineFunction &MF,
+                      bool IsVarArg,
+                      const SmallVectorImpl<ISD::OutputArg> &Outs,
+                      LLVMContext &Context) const override;
   SDValue LowerReturn(SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
                       const SmallVectorImpl<ISD::OutputArg> &Outs,
                       const SmallVectorImpl<SDValue> &OutVals, const SDLoc &DL,
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
index b8cc83a5d3e..ccdc7c60868 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -378,7 +378,10 @@ let isCall = 1, Defs = [X1_32] in
 def PseudoCALL : Pseudo<(outs), (ins GPR:$rs1), [(Call GPR:$rs1)]>,
                  PseudoInstExpansion<(JALR X1_32, GPR:$rs1, 0)>;
 
-let isBarrier = 1, isReturn = 1, isTerminator = 1 in
+// FIXME: PseudoRET doesn't really modify X1_32 (RA), however marking it as
+// such does ensure the the return address is always given a stack slot.
+
+let Defs = [X1_32], isBarrier = 1, isReturn = 1, isTerminator = 1 in
 def PseudoRET : Pseudo<(outs), (ins), [(RetFlag)]>,
                 PseudoInstExpansion<(JALR X0_32, X1_32, 0)>;
 
diff --git a/test/CodeGen/RISCV/byval.ll b/test/CodeGen/RISCV/byval.ll
new file mode 100644
index 00000000000..bef084f8376
--- /dev/null
+++ b/test/CodeGen/RISCV/byval.ll
@@ -0,0 +1,27 @@
+; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s | FileCheck %s
+
+%struct.Foo = type { i32, i32, i32, i16, i8 }
+@foo = global %struct.Foo { i32 1, i32 2, i32 3, i16 4, i8 5 }, align 4
+
+define i32 @callee(%struct.Foo* byval %f) nounwind {
+entry:
+; CHECK-LABEL: callee:
+; CHECK: lw a0, 0(a0)
+  %0 = getelementptr inbounds %struct.Foo, %struct.Foo* %f, i32 0, i32 0
+  %1 = load i32, i32* %0, align 4
+  ret i32 %1
+}
+
+
+define void @caller() nounwind {
+entry:
+; CHECK-LABEL: caller:
+; CHECK: lui a0, %hi(foo)
+; CHECK: addi a0, a0, %lo(foo)
+; CHECK: lw a0, 0(a0)
+; CHECK: sw a0, -24(s0)
+; CHECK: addi a0, s0, -24
+; CHECK-NEXT: jalr
+  %call = call i32 @callee(%struct.Foo* byval @foo)
+  ret void
+}
diff --git a/test/CodeGen/RISCV/calling-conv.ll b/test/CodeGen/RISCV/calling-conv.ll
new file mode 100644
index 00000000000..289c033b196
--- /dev/null
+++ b/test/CodeGen/RISCV/calling-conv.ll
@@ -0,0 +1,513 @@
+; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s | FileCheck %s
+
+; As well as calling convention details, we check that ra and fp are
+; consistently stored to fp-4 and fp-8.
+
+; Check that on RV32, i64 and double are passed in a pair of registers. Unlike
+; the convention for varargs, this need not be an aligned pair.
+
+define i32 @callee_scalars(i32 %a, i64 %b, i32 %c, i32 %d, double %e) {
+; CHECK-LABEL: callee_scalars:
+; CHECK: addi sp, sp, -32
+; CHECK: sw ra, 28(sp)
+; CHECK: sw s0, 24(sp)
+; CHECK: addi s0, sp, 32
+; CHECK: addi s1, a4, 0
+; CHECK: addi s2, a3, 0
+; CHECK: addi s3, a1, 0
+; CHECK: addi s4, a0, 0
+; CHECK: lui a0, %hi(__fixdfsi)
+; CHECK: addi a2, a0, %lo(__fixdfsi)
+; CHECK: addi a0, a5, 0
+; CHECK: addi a1, a6, 0
+; CHECK: jalr ra, a2, 0
+; CHECK: add a1, s4, s3
+; CHECK: add a1, a1, s2
+; CHECK: add a1, a1, s1
+; CHECK: add a0, a1, a0
+  %b_trunc = trunc i64 %b to i32
+  %e_fptosi = fptosi double %e to i32
+  %1 = add i32 %a, %b_trunc
+  %2 = add i32 %1, %c
+  %3 = add i32 %2, %d
+  %4 = add i32 %3, %e_fptosi
+  ret i32 %4
+}
+
+define i32 @caller_scalars() {
+; CHECK-LABEL: caller_scalars:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a0, 262464
+; CHECK: addi a6, a0, 0
+; CHECK: lui a0, %hi(callee_scalars)
+; CHECK: addi a7, a0, %lo(callee_scalars)
+; CHECK: addi a0, zero, 1
+; CHECK: addi a1, zero, 2
+; CHECK: addi a3, zero, 3
+; CHECK: addi a4, zero, 4
+; CHECK: addi a2, zero, 0
+; CHECK: addi a5, zero, 0
+; CHECK: jalr ra, a7, 0
+  %1 = call i32 @callee_scalars(i32 1, i64 2, i32 3, i32 4, double 5.000000e+00)
+  ret i32 %1
+}
+
+; Check that i128 and fp128 are passed indirectly
+
+define i32 @callee_large_scalars(i128 %a, fp128 %b) {
+; CHECK-LABEL: callee_large_scalars:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi  s0, sp, 16
+; CHECK: lw a2, 12(a1)
+; CHECK: lw a3, 12(a0)
+; CHECK: xor a2, a3, a2
+; CHECK: lw a3, 4(a1)
+; CHECK: lw a4, 4(a0)
+; CHECK: xor a3, a4, a3
+; CHECK: or a2, a3, a2
+; CHECK: lw a3, 8(a1)
+; CHECK: lw a4, 8(a0)
+; CHECK: xor a3, a4, a3
+; CHECK: lw a1, 0(a1)
+; CHECK: lw a0, 0(a0)
+; CHECK: xor a0, a0, a1
+; CHECK: or a0, a0, a3
+; CHECK: or a0, a0, a2
+; CHECK: xor a0, a0, zero
+; CHECK: sltiu a0, a0, 1
+  %b_bitcast = bitcast fp128 %b to i128
+  %1 = icmp eq i128 %a, %b_bitcast
+  %2 = zext i1 %1 to i32
+  ret i32 %2
+}
+
+define i32 @caller_large_scalars() {
+; CHECK-LABEL: caller_large_scalars:
+; CHECK: addi sp, sp, -48
+; CHECK: sw ra, 44(sp)
+; CHECK: sw s0, 40(sp)
+; CHECK: addi  s0, sp, 48
+; CHECK: sw zero, -40(s0)
+; CHECK: sw zero, -44(s0)
+; CHECK: sw zero, -48(s0)
+; CHECK: sw zero, -12(s0)
+; CHECK: sw zero, -16(s0)
+; CHECK: sw zero, -20(s0)
+; CHECK: addi a0, zero, 1
+; CHECK: sw a0, -24(s0)
+; CHECK: lui a0, 524272
+; CHECK: addi a0, a0, 0
+; CHECK: sw a0, -36(s0)
+; CHECK: lui a0, %hi(callee_large_scalars)
+; CHECK: addi a2, a0, %lo(callee_large_scalars)
+; CHECK: addi a0, s0, -24
+; CHECK: addi a1, s0, -48
+; CHECK: jalr ra, a2, 0
+  %1 = call i32 @callee_large_scalars(i128 1, fp128 0xL00000000000000007FFF000000000000)
+  ret i32 %1
+}
+
+; Ensure that libcalls generated in the middle-end obey the calling convention
+
+define i32 @caller_mixed_scalar_libcalls(i64 %a) {
+; CHECK-LABEL: caller_mixed_scalar_libcalls:
+; CHECK: addi sp, sp, -32
+; CHECK: sw ra, 28(sp)
+; CHECK: sw s0, 24(sp)
+; CHECK: addi s0, sp, 32
+; CHECK: addi a2, a1, 0
+; CHECK: addi a1, a0, 0
+; CHECK: lui a0, %hi(__floatditf)
+; CHECK: addi a3, a0, %lo(__floatditf)
+; CHECK: addi a0, s0, -24
+; CHECK: jalr ra, a3, 0
+; CHECK: lw a0, -24(s0)
+; CHECK: lw s0, 24(sp)
+; CHECK: lw ra, 28(sp)
+; CHECK: addi sp, sp, 32
+; CHECK: jalr zero, ra, 0
+  %1 = sitofp i64 %a to fp128
+  %2 = bitcast fp128 %1 to i128
+  %3 = trunc i128 %2 to i32
+  ret i32 %3
+}
+
+; Check that the stack is used once the GPRs are exhausted
+
+define i32 @callee_many_scalars(i8 %a, i16 %b, i32 %c, i64 %d, i32 %e, i32 %f, i64 %g, i32 %h) {
+; CHECK-LABEL: callee_many_scalars:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lw t0, 0(s0)
+; CHECK: xor a4, a4, t0
+; CHECK: xor a3, a3, a7
+; CHECK: or a3, a3, a4
+; CHECK: xor a3, a3, zero
+; CHECK: lui a4, 16
+; CHECK: addi a4, a4, -1
+; CHECK: and a1, a1, a4
+; CHECK: andi a0, a0, 255
+; CHECK: add a0, a0, a1
+; CHECK: add a0, a0, a2
+; CHECK: sltiu a1, a3, 1
+; CHECK: add a0, a1, a0
+; CHECK: add a0, a0, a5
+; CHECK: add a0, a0, a6
+; CHECK: lw a1, 4(s0)
+; CHECK: add a0, a0, a1
+  %a_ext = zext i8 %a to i32
+  %b_ext = zext i16 %b to i32
+  %1 = add i32 %a_ext, %b_ext
+  %2 = add i32 %1, %c
+  %3 = icmp eq i64 %d, %g
+  %4 = zext i1 %3 to i32
+  %5 = add i32 %4, %2
+  %6 = add i32 %5, %e
+  %7 = add i32 %6, %f
+  %8 = add i32 %7, %h
+  ret i32 %8
+}
+
+define i32 @caller_many_scalars() {
+; CHECK-LABEL: caller_many_scalars:
+; CHECK: addi sp, sp, -32
+; CHECK: sw ra, 28(sp)
+; CHECK: sw s0, 24(sp)
+; CHECK: addi s0, sp, 32
+; CHECK: addi a0, zero, 8
+; CHECK: sw a0, 4(sp)
+; CHECK: sw zero, 0(sp)
+; CHECK: lui a0, %hi(callee_many_scalars)
+; CHECK: addi t0, a0, %lo(callee_many_scalars)
+; CHECK: addi a0, zero, 1
+; CHECK: addi a1, zero, 2
+; CHECK: addi a2, zero, 3
+; CHECK: addi a3, zero, 4
+; CHECK: addi a5, zero, 5
+; CHECK: addi a6, zero, 6
+; CHECK: addi a7, zero, 7
+; CHECK: addi a4, zero, 0
+; CHECK: jalr ra, t0, 0
+  %1 = call i32 @callee_many_scalars(i8 1, i16 2, i32 3, i64 4, i32 5, i32 6, i64 7, i32 8)
+  ret i32 %1
+}
+
+; Check passing of coerced integer arrays
+
+%struct.small = type { i32, i32* }
+
+define i32 @callee_small_coerced_struct([2 x i32] %a.coerce) {
+; CHECK-LABEL: callee_small_coerced_struct:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: xor a0, a0, a1
+; CHECK: sltiu a0, a0, 1
+  %1 = extractvalue [2 x i32] %a.coerce, 0
+  %2 = extractvalue [2 x i32] %a.coerce, 1
+  %3 = icmp eq i32 %1, %2
+  %4 = zext i1 %3 to i32
+  ret i32 %4
+}
+
+define i32 @caller_small_coerced_struct() {
+; CHECK-LABEL: caller_small_coerced_struct:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a0, %hi(callee_small_coerced_struct)
+; CHECK: addi a2, a0, %lo(callee_small_coerced_struct)
+; CHECK: addi a0, zero, 1
+; CHECK: addi a1, zero, 2
+; CHECK: jalr ra, a2, 0
+  %1 = call i32 @callee_small_coerced_struct([2 x i32] [i32 1, i32 2])
+  ret i32 %1
+}
+
+; Check large struct arguments, which are passed byval
+
+%struct.large = type { i32, i32, i32, i32 }
+
+define i32 @callee_large_struct(%struct.large* byval align 4 %a) {
+; CHECK-LABEL: callee_large_struct:
+; CHECK: addi  sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lw a1, 12(a0)
+; CHECK: lw a0, 0(a0)
+; CHECK: add a0, a0, a1
+  %1 = getelementptr inbounds %struct.large, %struct.large* %a, i32 0, i32 0
+  %2 = getelementptr inbounds %struct.large, %struct.large* %a, i32 0, i32 3
+  %3 = load i32, i32* %1
+  %4 = load i32, i32* %2
+  %5 = add i32 %3, %4
+  ret i32 %5
+}
+
+define i32 @caller_large_struct() {
+; CHECK-LABEL: caller_large_struct:
+; CHECK: addi sp, sp, -48
+; CHECK: sw ra, 44(sp)
+; CHECK: sw s0, 40(sp)
+; CHECK: addi s0, sp, 48
+; CHECK: addi a0, zero, 1
+; CHECK: sw a0, -24(s0)
+; CHECK: sw a0, -40(s0)
+; CHECK: addi a0, zero, 2
+; CHECK: sw a0, -20(s0)
+; CHECK: sw a0, -36(s0)
+; CHECK: addi a0, zero, 3
+; CHECK: sw a0, -16(s0)
+; CHECK: sw a0, -32(s0)
+; CHECK: addi a0, zero, 4
+; CHECK: sw a0, -12(s0)
+; CHECK: sw a0, -28(s0)
+; CHECK: lui a0, %hi(callee_large_struct)
+; CHECK: addi a1, a0, %lo(callee_large_struct)
+; CHECK: addi a0, s0, -40
+; CHECK: jalr ra, a1, 0
+  %ls = alloca %struct.large, align 4
+  %1 = bitcast %struct.large* %ls to i8*
+  %a = getelementptr inbounds %struct.large, %struct.large* %ls, i32 0, i32 0
+  store i32 1, i32* %a
+  %b = getelementptr inbounds %struct.large, %struct.large* %ls, i32 0, i32 1
+  store i32 2, i32* %b
+  %c = getelementptr inbounds %struct.large, %struct.large* %ls, i32 0, i32 2
+  store i32 3, i32* %c
+  %d = getelementptr inbounds %struct.large, %struct.large* %ls, i32 0, i32 3
+  store i32 4, i32* %d
+  %2 = call i32 @callee_large_struct(%struct.large* byval align 4 %ls)
+  ret i32 %2
+}
+
+; Check 2x*xlen values are aligned appropriately when passed on the stack
+define i32 @callee_aligned_stack(i32 %a, i32 %b, fp128 %c, i32 %d, i32 %e,
+    i64 %f, i32 %g, i32 %h, double %i, i32 %j, [2 x i32] %k) {
+; The double should be 8-byte aligned on the stack, but the two-element array
+; should only be 4-byte aligned
+; CHECK-LABEL: callee_aligned_stack:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lw a0, 0(a2)
+; CHECK: add a0, a0, a7
+; CHECK: lw a1, 0(s0)
+; CHECK: add a0, a0, a1
+; CHECK: lw a1, 8(s0)
+; CHECK: add a0, a0, a1
+; CHECK: lw a1, 16(s0)
+; CHECK: add a0, a0, a1
+; CHECK: lw a1, 20(s0)
+; CHECK: add a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = bitcast fp128 %c to i128
+  %2 = trunc i128 %1 to i32
+  %3 = add i32 %2, %g
+  %4 = add i32 %3, %h
+  %5 = bitcast double %i to i64
+  %6 = trunc i64 %5 to i32
+  %7 = add i32 %4, %6
+  %8 = add i32 %7, %j
+  %9 = extractvalue [2 x i32] %k, 0
+  %10 = add i32 %8, %9
+  ret i32 %10
+}
+
+define void @caller_aligned_stack() {
+; The double should be 8-byte aligned on the stack, but the two-element array
+; should only be 4-byte aligned
+; CHECK-LABEL: caller_aligned_stack:
+; CHECK: addi sp, sp, -64
+; CHECK: sw ra, 60(sp)
+; CHECK: sw s0, 56(sp)
+; CHECK: addi s0, sp, 64
+; CHECK: addi a0, zero, 18
+; CHECK: sw a0, 24(sp)
+; CHECK: addi a0, zero, 17
+; CHECK: sw a0, 20(sp)
+; CHECK: addi a0, zero, 16
+; CHECK: sw a0, 16(sp)
+; CHECK: lui a0, 262236
+; CHECK: addi a0, a0, 655
+; CHECK: sw a0, 12(sp)
+; CHECK: lui a0, 377487
+; CHECK: addi a0, a0, 1475
+; CHECK: sw a0, 8(sp)
+; CHECK: addi a0, zero, 15
+; CHECK: sw a0, 0(sp)
+  %1 = call i32 @callee_aligned_stack(i32 1, i32 11,
+    fp128 0xLEB851EB851EB851F400091EB851EB851, i32 12, i32 13,
+    i64 20000000000, i32 14, i32 15, double 2.720000e+00, i32 16,
+    [2 x i32] [i32 17, i32 18])
+  ret void
+}
+
+; Check return of 2x xlen scalars
+
+define i64 @callee_small_scalar_ret() {
+; CHECK-LABEL: callee_small_scalar_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a0, 466866
+; CHECK: addi a0, a0, 1677
+; CHECK: addi a1, zero, 287
+  ret i64 1234567898765
+}
+
+define i32 @caller_small_scalar_ret() {
+; CHECK-LABEL: caller_small_scalar_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a0, %hi(callee_small_scalar_ret)
+; CHECK: addi a0, a0, %lo(callee_small_scalar_ret)
+; CHECK: jalr ra, a0, 0
+; CHECK: lui a2, 56
+; CHECK: addi a2, a2, 580
+; CHECK: xor a1, a1, a2
+; CHECK: lui a2, 200614
+; CHECK: addi a2, a2, 647
+; CHECK: xor a0, a0, a2
+; CHECK: or a0, a0, a1
+; CHECK: xor a0, a0, zero
+; CHECK: sltiu a0, a0, 1
+  %1 = call i64 @callee_small_scalar_ret()
+  %2 = icmp eq i64 987654321234567, %1
+  %3 = zext i1 %2 to i32
+  ret i32 %3
+}
+
+; Check return of 2x xlen structs
+
+define %struct.small @callee_small_struct_ret() {
+; CHECK-LABEL: callee_small_struct_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: addi a0, zero, 1
+; CHECK: addi a1, zero, 0
+  ret %struct.small { i32 1, i32* null }
+}
+
+define i32 @caller_small_struct_ret() {
+; CHECK-LABEL: caller_small_struct_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a0, %hi(callee_small_struct_ret)
+; CHECK: addi a0, a0, %lo(callee_small_struct_ret)
+; CHECK: jalr ra, a0, 0
+; CHECK: add a0, a0, a1
+; CHECK: lw s0, 8(sp)
+; CHECK: lw ra, 12(sp)
+; CHECK: addi sp, sp, 16
+; CHECK: jalr zero, ra, 0
+  %1 = call %struct.small @callee_small_struct_ret()
+  %2 = extractvalue %struct.small %1, 0
+  %3 = extractvalue %struct.small %1, 1
+  %4 = ptrtoint i32* %3 to i32
+  %5 = add i32 %2, %4
+  ret i32 %5
+}
+
+; Check return of >2x xlen scalars
+
+define fp128 @callee_large_scalar_ret() {
+; CHECK-LABEL: callee_large_scalar_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: lui a1, 524272
+; CHECK: addi a1, a1, 0
+; CHECK: sw a1, 12(a0)
+; CHECK: sw zero, 8(a0)
+; CHECK: sw zero, 4(a0)
+; CHECK: sw zero, 0(a0)
+; CHECK: jalr zero, ra, 0
+  ret fp128 0xL00000000000000007FFF000000000000
+}
+
+define void @caller_large_scalar_ret() {
+; CHECK-LABEL: caller_large_scalar_ret:
+; CHECK: addi sp, sp, -32
+; CHECK: sw ra, 28(sp)
+; CHECK: sw s0, 24(sp)
+; CHECK: addi s0, sp, 32
+; CHECK: lui a0, %hi(callee_large_scalar_ret)
+; CHECK: addi a1, a0, %lo(callee_large_scalar_ret)
+; CHECK: addi a0, s0, -32
+; CHECK: jalr ra, a1, 0
+  %1 = call fp128 @callee_large_scalar_ret()
+  ret void
+}
+
+; Check return of >2x xlen structs
+
+define void @callee_large_struct_ret(%struct.large* noalias sret %agg.result) {
+; CHECK-LABEL: callee_large_struct_ret:
+; CHECK: addi sp, sp, -16
+; CHECK: sw ra, 12(sp)
+; CHECK: sw s0, 8(sp)
+; CHECK: addi s0, sp, 16
+; CHECK: addi a1, zero, 2
+; CHECK: sw a1, 4(a0)
+; CHECK: addi a1, zero, 1
+; CHECK: sw a1, 0(a0)
+; CHECK: addi a1, zero, 3
+; CHECK: sw a1, 8(a0)
+; CHECK: addi a1, zero, 4
+; CHECK: sw a1, 12(a0)
+  %a = getelementptr inbounds %struct.large, %struct.large* %agg.result, i32 0, i32 0
+  store i32 1, i32* %a, align 4
+  %b = getelementptr inbounds %struct.large, %struct.large* %agg.result, i32 0, i32 1
+  store i32 2, i32* %b, align 4
+  %c = getelementptr inbounds %struct.large, %struct.large* %agg.result, i32 0, i32 2
+  store i32 3, i32* %c, align 4
+  %d = getelementptr inbounds %struct.large, %struct.large* %agg.result, i32 0, i32 3
+  store i32 4, i32* %d, align 4
+  ret void
+}
+
+define i32 @caller_large_struct_ret() {
+; CHECK-LABEL: caller_large_struct_ret:
+; CHECK: addi sp, sp, -32
+; CHECK: sw ra, 28(sp)
+; CHECK: sw s0, 24(sp)
+; CHECK: addi s0, sp, 32
+; CHECK: lui a0, %hi(callee_large_struct_ret)
+; CHECK: addi a1, a0, %lo(callee_large_struct_ret)
+; CHECK: addi a0, s0, -24
+; CHECK: jalr ra, a1, 0
+; CHECK: lw a0, -12(s0)
+; CHECK: lw a1, -24(s0)
+; CHECK: add a0, a1, a0
+; CHECK: lw s0, 24(sp)
+; CHECK: lw ra, 28(sp)
+; CHECK: addi sp, sp, 32
+; CHECK: jalr zero, ra, 0
+  %1 = alloca %struct.large
+  call void @callee_large_struct_ret(%struct.large* sret %1)
+  %2 = getelementptr inbounds %struct.large, %struct.large* %1, i32 0, i32 0
+  %3 = load i32, i32* %2
+  %4 = getelementptr inbounds %struct.large, %struct.large* %1, i32 0, i32 3
+  %5 = load i32, i32* %4
+  %6 = add i32 %3, %5
+  ret i32 %6
+}
diff --git a/test/CodeGen/RISCV/calls.ll b/test/CodeGen/RISCV/calls.ll
index ec490696ccc..20f7381064d 100644
--- a/test/CodeGen/RISCV/calls.ll
+++ b/test/CodeGen/RISCV/calls.ll
@@ -47,3 +47,30 @@ define i32 @test_call_fastcc(i32 %a, i32 %b) {
   %1 = call fastcc i32 @fastcc_function(i32 %a, i32 %b)
   ret i32 %a
 }
+
+declare i32 @external_many_args(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) nounwind
+
+define i32 @test_call_external_many_args(i32 %a) {
+; CHECK-LABEL: test_call_external_many_args:
+; CHECK: lui a0, %hi(external_many_args)
+; CHECK: addi t0, a0, %lo(external_many_args)
+; CHECK: jalr ra, t0, 0
+  %1 = call i32 @external_many_args(i32 %a, i32 %a, i32 %a, i32 %a, i32 %a,
+                                    i32 %a, i32 %a, i32 %a, i32 %a, i32 %a)
+  ret i32 %a
+}
+
+define i32 @defined_many_args(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32 %j) {
+  %added = add i32 %j, 1
+  ret i32 %added
+}
+
+define i32 @test_call_defined_many_args(i32 %a) {
+; CHECK-LABEL: test_call_defined_many_args:
+; CHECK: lui a1, %hi(defined_many_args)
+; CHECK: addi t0, a1, %lo(defined_many_args)
+; CHECK: jalr ra, t0, 0
+  %1 = call i32 @defined_many_args(i32 %a, i32 %a, i32 %a, i32 %a, i32 %a,
+                                   i32 %a, i32 %a, i32 %a, i32 %a, i32 %a)
+  ret i32 %1
+}
diff --git a/test/CodeGen/RISCV/fp128.ll b/test/CodeGen/RISCV/fp128.ll
new file mode 100644
index 00000000000..8c0f72996e6
--- /dev/null
+++ b/test/CodeGen/RISCV/fp128.ll
@@ -0,0 +1,40 @@
+; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s | FileCheck %s
+
+@x = local_unnamed_addr global fp128 0xL00000000000000007FFF000000000000, align 16
+@y = local_unnamed_addr global fp128 0xL00000000000000007FFF000000000000, align 16
+
+; Besides anything else, these tests help verify that libcall ABI lowering
+; works correctly
+
+define i32 @test_load_and_cmp() {
+; CHECK-LABEL: test_load_and_cmp:
+; CHECK: lui a0, %hi(__netf2)
+; CHECK: addi a2, a0, %lo(__netf2)
+; CHECK: addi a0, s0, -24
+; CHECK: addi a1, s0, -40
+; CHECK: jalr ra, a2, 0
+  %1 = load fp128, fp128* @x, align 16
+  %2 = load fp128, fp128* @y, align 16
+  %cmp = fcmp une fp128 %1, %2
+  %3 = zext i1 %cmp to i32
+  ret i32 %3
+}
+
+define i32 @test_add_and_fptosi() {
+; CHECK-LABEL: test_add_and_fptosi:
+; CHECK: lui a0, %hi(__addtf3)
+; CHECK: addi a3, a0, %lo(__addtf3)
+; CHECK: addi a0, s0, -24
+; CHECK: addi a1, s0, -40
+; CHECK: addi a2, s0, -56
+; CHECK: jalr ra, a3, 0
+; CHECK: lui a0, %hi(__fixtfsi)
+; CHECK: addi a1, a0, %lo(__fixtfsi)
+; CHECK: addi a0, s0, -72
+; CHECK: jalr ra, a1, 0
+  %1 = load fp128, fp128* @x, align 16
+  %2 = load fp128, fp128* @y, align 16
+  %3 = fadd fp128 %1, %2
+  %4 = fptosi fp128 %3 to i32
+  ret i32 %4
+}
-- 
2.14.1

