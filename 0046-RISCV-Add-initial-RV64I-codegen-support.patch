From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Add initial RV64I codegen support

Most of RISCVTargetLowering is now parameterised by XLEN.

Note this patch contains a temporary workaround in CodeGenDAGPatterns.cpp.

This patch also reformats RISCVInstrInfo.td to ensure consistent indentation
and naming (TODO: back-propagate these changes).
---
 lib/Target/RISCV/RISCV.td                 |   1 +
 lib/Target/RISCV/RISCVISelLowering.cpp    |   9 +-
 lib/Target/RISCV/RISCVInstrInfo.cpp       |   7 +-
 lib/Target/RISCV/RISCVInstrInfo.td        |  79 ++++++--
 lib/Target/RISCV/RISCVRegisterInfo.cpp    |  17 +-
 lib/Target/RISCV/RISCVTargetMachine.cpp   |   2 +-
 test/CodeGen/RISCV/addc-adde-sube-subc.ll |  14 ++
 test/CodeGen/RISCV/alu64.ll               | 288 ++++++++++++++++++++++++++++++
 test/CodeGen/RISCV/mem64.ll               | 198 ++++++++++++++++++++
 utils/TableGen/CodeGenDAGPatterns.cpp     |   4 +-
 10 files changed, 591 insertions(+), 28 deletions(-)
 create mode 100644 test/CodeGen/RISCV/alu64.ll
 create mode 100644 test/CodeGen/RISCV/mem64.ll

diff --git a/lib/Target/RISCV/RISCV.td b/lib/Target/RISCV/RISCV.td
index 84715a3a2d0..fee8dde3f40 100644
--- a/lib/Target/RISCV/RISCV.td
+++ b/lib/Target/RISCV/RISCV.td
@@ -38,6 +38,7 @@ def Feature64Bit   : SubtargetFeature<"64bit", "HasRV64", "true",
                            "Implements RV64">;
 def IsRV64         :  Predicate<"Subtarget->is64Bit()">,
                            AssemblerPredicate<"Feature64Bit">;
+def IsRV32         :  Predicate<"!Subtarget->is64Bit()">;
 
 def RV64           : HwMode<"+64bit">;
 def RV32           : HwMode<"-64bit">;
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index a988fbc7097..9ff9af9a664 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -183,7 +183,7 @@ SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,
   const GlobalValue *GV = N->getGlobal();
   int64_t Offset = N->getOffset();
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue GAHi =
         DAG.getTargetGlobalAddress(GV, DL, Ty, Offset, RISCVII::MO_HI);
     SDValue GALo =
@@ -205,7 +205,7 @@ SDValue RISCVTargetLowering::lowerBlockAddress(SDValue Op,
   const BlockAddress *BA = N->getBlockAddress();
   int64_t Offset = N->getOffset();
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue BAHi = DAG.getTargetBlockAddress(BA, Ty, Offset, RISCVII::MO_HI);
     SDValue BALo = DAG.getTargetBlockAddress(BA, Ty, Offset, RISCVII::MO_LO);
     SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, Ty, BAHi), 0);
@@ -226,7 +226,7 @@ SDValue RISCVTargetLowering::lowerExternalSymbol(SDValue Op,
 
   // TODO: should also handle gp-relative loads
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue GAHi = DAG.getTargetExternalSymbol(Sym, Ty, RISCVII::MO_HI);
     SDValue GALo = DAG.getTargetExternalSymbol(Sym, Ty, RISCVII::MO_LO);
     SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, Ty, GAHi), 0);
@@ -617,9 +617,8 @@ SDValue RISCVTargetLowering::LowerFormalArguments(
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MachineRegisterInfo &RegInfo = MF.getRegInfo();
   MVT XLenVT = Subtarget.getXLenVT();
+  unsigned XLenInBytes = Subtarget.getXLen()/8;
   EVT PtrVT = getPointerTy(DAG.getDataLayout());
-  unsigned XLen = DAG.getDataLayout().getLargestLegalIntTypeSizeInBits();
-  unsigned XLenInBytes = XLen / 8;
   // Used with vargs to acumulate store chains.
   std::vector<SDValue> OutChains;
 
diff --git a/lib/Target/RISCV/RISCVInstrInfo.cpp b/lib/Target/RISCV/RISCVInstrInfo.cpp
index 13babf7c710..7c1e9fddff5 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.cpp
+++ b/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -57,7 +57,7 @@ void RISCVInstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
     DL = I->getDebugLoc();
 
   if (RC == &RISCV::GPRRegClass)
-    BuildMI(MBB, I, DL, get(RISCV::SW_FI))
+    BuildMI(MBB, I, DL, get(RISCV::StXLEN_FI))
         .addReg(SrcReg, getKillRegState(IsKill))
         .addFrameIndex(FI)
         .addImm(0);
@@ -75,7 +75,7 @@ void RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
     DL = I->getDebugLoc();
 
   if (RC == &RISCV::GPRRegClass)
-    BuildMI(MBB, I, DL, get(RISCV::LW_FI), DestReg).addFrameIndex(FI).addImm(0);
+    BuildMI(MBB, I, DL, get(RISCV::LdXLEN_FI), DestReg).addFrameIndex(FI).addImm(0);
   else
     llvm_unreachable("Can't load this register from stack slot");
 }
@@ -289,9 +289,8 @@ unsigned RISCVInstrInfo::insertIndirectBranch(MachineBasicBlock &MBB,
   MachineFunction *MF = MBB.getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const auto &TM = static_cast<const RISCVTargetMachine &>(MF->getTarget());
-  const auto &STI = MF->getSubtarget<RISCVSubtarget>();
 
-  if (TM.isPositionIndependent() || STI.is64Bit())
+  if (TM.isPositionIndependent())
     report_fatal_error("Unable to insert indirect branch");
 
   if (!isInt<32>(BrOffset))
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
index 6c632043fc0..4e60d810d01 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -26,7 +26,6 @@ def SDT_RISCVSelectCC     : SDTypeProfile<1, 5, [SDTCisSameAs<1, 2>,
                                                  SDTCisSameAs<0, 4>,
                                                  SDTCisSameAs<4, 5>]>;
 
-
 def Call         : SDNode<"RISCVISD::CALL", SDT_RISCVCall,
                           [SDNPHasChain, SDNPOptInGlue, SDNPOutGlue,
                            SDNPVariadic]>;
@@ -126,6 +125,7 @@ def ixlenimm : Operand<XLenVT>;
 
 // Standalone (codegen-only) immleaf patterns.
 def simm32 : ImmLeaf<XLenVT, [{return isInt<32>(Imm);}]>;
+def simm64 : ImmLeaf<XLenVT, [{return isInt<64>(Imm);}]>;
 
 // Addressing modes.
 def ADDRii : ComplexPattern<iPTR, 2, "SelectADDRii", [add, frameindex], []>;
@@ -150,6 +150,23 @@ def HI20 : SDNodeXForm<imm, [{
                                    SDLoc(N), N->getValueType(0));
 }]>;
 
+// Extract least significant 12 bits from the upper half of an immediate value
+// and sign extend them.
+def HLO12Sext : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(SignExtend64<12>(Val),
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
+// Extract the most significant 20 bits from the upper half of an immediate
+// value. Add 1 if bit 32+11 is 1, to compensate for the low 12 bits in the
+// matching immediate addi or ld/st being negative.
+def HHI20 : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(((Val+0x800) >> 12) & 0xfffff,
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
 //===----------------------------------------------------------------------===//
 // Instruction Class Templates
 //===----------------------------------------------------------------------===//
@@ -358,7 +375,8 @@ class PatGprUimmLog2XLen<SDPatternOperator OpNode, RVInstIShift Inst>
 /// Immediates
 
 def : Pat<(simm12:$imm), (ADDI X0, simm12:$imm)>;
-def : Pat<(simm32:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+def : Pat<(simm32:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>,
+      Requires<[IsRV32]>;
 
 /// Simple arithmetic operations
 
@@ -380,7 +398,8 @@ def : PatGprUimmLog2XLen<sra, SRAI>;
 
 // Add with a frameindex, used to legalize frameindex copies and necessary to
 // keep tblgen happy
-def LEA_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr), [(set GPR:$dst, ADDRii:$addr)]>;
+def LEA_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr),
+                    [(set GPR:$dst, ADDRii:$addr)]>;
 
 /// Setcc
 
@@ -473,13 +492,12 @@ defm : LdPat<sextloadi8, LB>;
 defm : LdPat<extloadi8, LB>;
 defm : LdPat<sextloadi16, LH>;
 defm : LdPat<extloadi16, LH>;
-defm : LdPat<load, LW>;
+defm : LdPat<load, LW>, Requires<[IsRV32]>;
 defm : LdPat<zextloadi8, LBU>;
 defm : LdPat<zextloadi16, LHU>;
 
-def LW_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr),
-                   [(set GPR:$dst, (load ADDRii:$addr))]>;
-
+def LdXLEN_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr),
+                       [(set GPR:$dst, (load ADDRii:$addr))]>;
 
 /// Stores
 
@@ -491,10 +509,10 @@ multiclass StPat<PatFrag StoreOp, RVInst Inst> {
 
 defm : StPat<truncstorei8, SB>;
 defm : StPat<truncstorei16, SH>;
-defm : StPat<store, SW>;
+defm : StPat<store, SW>, Requires<[IsRV32]>;
 
-def SW_FI : Pseudo<(outs), (ins GPR:$src, MEMii:$addr),
-                   [(store GPR:$src, ADDRii:$addr)]>;
+def StXLEN_FI : Pseudo<(outs), (ins GPR:$src, MEMii:$addr),
+                       [(store GPR:$src, ADDRii:$addr)]>;
 
 /// Other pseudo-instructions
 
@@ -506,6 +524,47 @@ def ADJCALLSTACKUP   : Pseudo<(outs), (ins i32imm:$amt1, i32imm:$amt2),
                               [(CallSeqEnd timm:$amt1, timm:$amt2)]>;
 } // Defs = [X2], Uses = [X2]
 
+/// RV64 patterns
+
+let Predicates = [IsRV64] in {
+def : Pat<(simm32:$imm), (ADDIW (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+// TODO: add better patterns for loading 64-bit immediates
+def : Pat<(simm64:$imm), (OR (SLLI (ADDIW (LUI (HHI20 imm:$imm)),
+                                           (HLO12Sext imm:$imm)),
+                                   (i64 32)),
+                             (SRLI (SLLI (ADDIW (LUI (HI20 imm:$imm)),
+                                          (LO12Sext imm:$imm)),
+                                    32),
+                              32))>;
+
+def : Pat<(sext_inreg (add GPR:$rs1, simm12:$imm12), i32),
+          (ADDIW GPR:$rs1, simm12:$imm12)>;
+// sext.w
+def : Pat<(sext_inreg GPR:$rs1, i32), (ADDIW GPR:$rs1, 0)>;
+
+def : Pat<(sext_inreg (shl GPR:$rs1, uimm5:$shamt), i32),
+          (SLLIW GPR:$rs1, uimm5:$shamt)>;
+// TODO: Patterns for SRLIW, SRAIW
+def : Pat<(sext_inreg (add GPR:$rs1, GPR:$rs2), i32),
+          (ADDW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (sub GPR:$rs1, GPR:$rs2), i32),
+          (SUBW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (shl GPR:$rs1, GPR:$rs2), i32),
+          (SLLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(srl (and GPR:$rs1, 0xffffffff), GPR:$rs2),
+          (SRLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sra (sext_inreg GPR:$rs1, i32), GPR:$rs2),
+          (SRAW GPR:$rs1, GPR:$rs2)>;
+
+defm : LdPat<sextloadi32, LW>;
+defm : LdPat<extloadi32, LW>;
+defm : LdPat<zextloadi32, LWU>;
+defm : LdPat<load, LD>;
+
+defm : StPat<truncstorei32, SW>;
+defm : StPat<store, SD>;
+} // Predicates = [IsRV64]
+
 //===----------------------------------------------------------------------===//
 // Standard extensions
 //===----------------------------------------------------------------------===//
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.cpp b/lib/Target/RISCV/RISCVRegisterInfo.cpp
index b55d823cce9..f3bb1a69147 100644
--- a/lib/Target/RISCV/RISCVRegisterInfo.cpp
+++ b/lib/Target/RISCV/RISCVRegisterInfo.cpp
@@ -62,6 +62,7 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   MachineInstr &MI = *II;
   MachineFunction &MF = *MI.getParent()->getParent();
+  const auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();
   RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MachineRegisterInfo &MRI = MF.getRegInfo();
@@ -125,14 +126,20 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
     FrameRegFlags = RegState::Kill;
   }
 
+  unsigned Opc;
+
   switch (MI.getOpcode()) {
-  case RISCV::LW_FI:
-    BuildMI(MBB, II, DL, TII->get(RISCV::LW), Reg)
+  default:
+    llvm_unreachable("Unexpected opcode");
+  case RISCV::LdXLEN_FI:
+    Opc = Subtarget.is64Bit() ? RISCV::LD : RISCV::LW;
+    BuildMI(MBB, II, DL, TII->get(Opc), Reg)
         .addReg(FrameReg, FrameRegFlags)
         .addImm(Offset);
     break;
-  case RISCV::SW_FI:
-    BuildMI(MBB, II, DL, TII->get(RISCV::SW))
+  case RISCV::StXLEN_FI:
+    Opc = Subtarget.is64Bit() ? RISCV::SD : RISCV::SW;
+    BuildMI(MBB, II, DL, TII->get(Opc))
         .addReg(Reg, getKillRegState(MI.getOperand(0).isKill()))
         .addReg(FrameReg, FrameRegFlags | RegState::Kill)
         .addImm(Offset);
@@ -142,8 +149,6 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
         .addReg(FrameReg, FrameRegFlags)
         .addImm(Offset);
     break;
-  default:
-    llvm_unreachable("Unexpected opcode");
   }
 
   // Erase old instruction.
diff --git a/lib/Target/RISCV/RISCVTargetMachine.cpp b/lib/Target/RISCV/RISCVTargetMachine.cpp
index 9b7fa590f91..e75fb3b701c 100644
--- a/lib/Target/RISCV/RISCVTargetMachine.cpp
+++ b/lib/Target/RISCV/RISCVTargetMachine.cpp
@@ -30,7 +30,7 @@ extern "C" void LLVMInitializeRISCVTarget() {
 
 static std::string computeDataLayout(const Triple &TT) {
   if (TT.isArch64Bit()) {
-    return "e-m:e-i64:64-n32:64-S128";
+    return "e-m:e-p:64:64-i64:64-i128:128-n64-S128";
   } else {
     assert(TT.isArch32Bit() && "only RV32 and RV64 are currently supported");
     return "e-m:e-p:32:32-i64:64-n32-S128";
diff --git a/test/CodeGen/RISCV/addc-adde-sube-subc.ll b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
index def5b76a79c..04d58c808e0 100644
--- a/test/CodeGen/RISCV/addc-adde-sube-subc.ll
+++ b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
@@ -24,3 +24,17 @@ define i64 @subc_sube(i64 %a, i64 %b) {
   %1 = sub i64 %a, %b
   ret i64 %1
 }
+
+define i128 @addc_adde128(i128 %a, i128 %b) {
+; CHECK-LABEL: addc_adde128:
+; TODO
+  %1 = add i128 %a, %b
+  ret i128 %1
+}
+
+define i128 @subc_sube128(i128 %a, i128 %b) {
+; CHECK-LABEL: subc_sube128:
+; TODO
+  %1 = sub i128 %a, %b
+  ret i128 %1
+}
diff --git a/test/CodeGen/RISCV/alu64.ll b/test/CodeGen/RISCV/alu64.ll
new file mode 100644
index 00000000000..07c405d5804
--- /dev/null
+++ b/test/CodeGen/RISCV/alu64.ll
@@ -0,0 +1,288 @@
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s | FileCheck %s
+
+; These tests are each targeted at a particular RISC-V RV64I ALU instruction. 
+; Other files in this folder exercise LLVM IR instructions that don't directly 
+; match RISC-V instruction
+
+; Register-immediate instructions
+
+define i64 @addi(i64 %a) nounwind {
+; CHECK-LABEL: addi:
+; CHECK: addi a0, a0, 1
+; CHECK: jalr zero, ra, 0
+; TODO: check support for materialising larger constants
+  %1 = add i64 %a, 1
+  ret i64 %1
+}
+
+define i64 @slti(i64 %a) nounwind {
+; CHECK-LABEL: slti:
+; CHECK: slti a0, a0, 2
+; CHECK: jalr zero, ra, 0
+  %1 = icmp slt i64 %a, 2
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltiu(i64 %a) nounwind {
+; CHECK-LABEL: sltiu:
+; CHECK: sltiu a0, a0, 3
+; CHECK: jalr zero, ra, 0
+  %1 = icmp ult i64 %a, 3
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xori(i64 %a) nounwind {
+; CHECK-LABEL: xori:
+; CHECK: xori a0, a0, 4
+; CHECK: jalr zero, ra, 0
+  %1 = xor i64 %a, 4
+  ret i64 %1
+}
+
+define i64 @ori(i64 %a) nounwind {
+; CHECK-LABEL: ori:
+; CHECK: ori a0, a0, 5
+; CHECK: jalr zero, ra, 0
+  %1 = or i64 %a, 5
+  ret i64 %1
+}
+
+define i64 @andi(i64 %a) nounwind {
+; CHECK-LABEL: andi:
+; CHECK: andi a0, a0, 6
+; CHECK: jalr zero, ra, 0
+  %1 = and i64 %a, 6
+  ret i64 %1
+}
+
+define i64 @slli(i64 %a) nounwind {
+; CHECK-LABEL: slli:
+; CHECK: slli a0, a0, 7
+; CHECK: jalr zero, ra, 0
+  %1 = shl i64 %a, 7
+  ret i64 %1
+}
+
+define i64 @srli(i64 %a) nounwind {
+; CHECK-LABEL: srli:
+; CHECK: srli a0, a0, 8
+; CHECK: jalr zero, ra, 0
+  %1 = lshr i64 %a, 8
+  ret i64 %1
+}
+
+define i64 @srai(i64 %a) nounwind {
+; CHECK-LABEL: srai:
+; CHECK: srai a0, a0, 9
+; CHECK: jalr zero, ra, 0
+  %1 = ashr i64 %a, 9
+  ret i64 %1
+}
+
+; Register-register instructions
+
+define i64 @add(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: add:
+; CHECK: add a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = add i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sub(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sub:
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = sub i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sll(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sll:
+; CHECK: sll a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = shl i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @slt(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: slt:
+; CHECK: slt a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = icmp slt i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltu(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sltu:
+; CHECK: sltu a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = icmp ult i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xor(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: xor:
+; CHECK: xor a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = xor i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @srl(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: srl:
+; CHECK: srl a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = lshr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sra(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sra:
+; CHECK: sra a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = ashr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @or(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: or:
+; CHECK: or a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = or i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @and(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: and:
+; CHECK: and a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = and i64 %a, %b
+  ret i64 %1
+}
+
+define signext i32 @addiw(i32 signext %a) {
+; CHECK-LABEL: addiw:
+; CHECK: addiw a0, a0, 123
+; CHECK: jalr zero, ra, 0
+  %1 = add i32 %a, 123
+  ret i32 %1
+}
+
+define signext i32 @slliw(i32 signext %a) {
+; CHECK-LABEL: slliw:
+; CHECK: slliw a0, a0, 17
+; CHECK: jalr zero, ra, 0
+  %1 = shl i32 %a, 17
+  ret i32 %1
+}
+
+define signext i32 @srliw(i32 signext %a) {
+; CHECK-LABEL: srliw:
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sraiw(i32 signext %a) {
+; CHECK-LABEL: sraiw:
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sextw(i32 zeroext %a) {
+; CHECK-LABEL: sextw:
+; CHECK: addiw a0, a0, 0
+  ret i32 %a
+}
+
+define signext i32 @addw(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: addw:
+; CHECK: addw a0, a0, a1
+  %1 = add i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @subw(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: subw:
+; CHECK: subw a0, a0, a1
+  %1 = sub i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sllw(i32 signext %a, i32 zeroext %b) {
+; CHECK-LABEL: sllw:
+; CHECK: sllw a0, a0, a1
+  %1 = shl i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @srlw(i32 signext %a, i32 zeroext %b) {
+; CHECK-LABEL: srlw:
+; CHECK: srlw a0, a0, a1
+  %1 = lshr i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sraw(i64 %a, i32 zeroext %b) {
+; CHECK-LABEL: sraw:
+; CHECK: sraw a0, a0, a1
+  %1 = trunc i64 %a to i32
+  %2 = ashr i32 %1, %b
+  ret i32 %2
+}
+
+; Materialize constants
+
+define i64 @zero() {
+; CHECK-LABEL: zero:
+; CHECK: addi a0, zero, 0
+; CHECK: jalr zero, ra, 0
+  ret i64 0
+}
+
+define i64 @pos_small() {
+; CHECK-LABEL: pos_small:
+; CHECK: addi a0, zero, 2047
+; CHECK: jalr zero, ra, 0
+  ret i64 2047
+}
+
+define i64 @neg_small() {
+; CHECK-LABEL: neg_small:
+; CHECK: addi a0, zero, -2048
+; CHECK: jalr zero, ra, 0
+  ret i64 -2048
+}
+
+define i64 @pos_imm32() {
+; CHECK-LABEL: pos_imm32:
+; CHECK: lui [[REG:[a-z0-9]+]], 423811
+; CHECK: addiw a0, [[REG]], -1297
+; CHECK: jalr zero, ra, 0
+  ret i64 1735928559
+}
+
+define i64 @neg_imm32() {
+; CHECK-LABEL: neg_imm32:
+; CHECK: lui [[REG:[a-z0-9]+]], 912092
+; CHECK: addiw a0, [[REG]], -273
+; CHECK: jalr zero, ra, 0
+  ret i64 -559038737
+}
+
+define i64 @pos_imm64() {
+; CHECK-LABEL: pos_imm64:
+; CHECK: lui a0, 7018
+; CHECK: addiw a0, a0, -1212
+; CHECK: slli  a0, a0, 32
+; CHECK: lui a1, 430355
+; CHECK: addiw a1, a1, 787
+; CHECK: slli  a1, a1, 32
+; CHECK: srli  a1, a1, 32
+; CHECK: or  a0, a0, a1
+  ret i64 123456757922083603
+}
diff --git a/test/CodeGen/RISCV/mem64.ll b/test/CodeGen/RISCV/mem64.ll
new file mode 100644
index 00000000000..2560b988ed3
--- /dev/null
+++ b/test/CodeGen/RISCV/mem64.ll
@@ -0,0 +1,198 @@
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s | FileCheck %s
+
+; Check indexed and unindexed, sext, zext and anyext loads
+
+define i64 @lb(i8 *%a) nounwind {
+; CHECK-LABEL: lb:
+; CHECK: lb a1, 0(a0)
+; CHECK: lb a0, 1(a0)
+  %1 = getelementptr i8, i8* %a, i32 1
+  %2 = load i8, i8* %1
+  %3 = sext i8 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i8, i8* %a
+  ret i64 %3
+}
+
+define i64 @lh(i16 *%a) nounwind {
+; CHECK-LABEL: lh:
+; CHECK: lh a1, 0(a0)
+; CHECK: lh a0, 4(a0)
+  %1 = getelementptr i16, i16* %a, i32 2
+  %2 = load i16, i16* %1
+  %3 = sext i16 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i16, i16* %a
+  ret i64 %3
+}
+
+define i64 @lw(i32 *%a) nounwind {
+; CHECK-LABEL: lw:
+; CHECK: lw a1, 0(a0)
+; CHECK: lw a0, 12(a0)
+  %1 = getelementptr i32, i32* %a, i32 3
+  %2 = load i32, i32* %1
+  %3 = sext i32 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i32, i32* %a
+  ret i64 %3
+}
+
+define i64 @lbu(i8 *%a) nounwind {
+; CHECK-LABEL: lbu:
+; CHECK: lbu a1, 0(a0)
+; CHECK: lbu a0, 4(a0)
+  %1 = getelementptr i8, i8* %a, i32 4
+  %2 = load i8, i8* %1
+  %3 = zext i8 %2 to i64
+  %4 = load volatile i8, i8* %a
+  %5 = zext i8 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lhu(i16 *%a) nounwind {
+; CHECK-LABEL: lhu:
+; CHECK: lhu a1, 0(a0)
+; CHECK: lhu a0, 10(a0)
+  %1 = getelementptr i16, i16* %a, i32 5
+  %2 = load i16, i16* %1
+  %3 = zext i16 %2 to i64
+  %4 = load volatile i16, i16* %a
+  %5 = zext i16 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lwu(i32 *%a) nounwind {
+; CHECK-LABEL: lwu:
+; CHECK: lwu a1, 0(a0)
+; CHECK: lwu a0, 24(a0)
+  %1 = getelementptr i32, i32* %a, i32 6
+  %2 = load i32, i32* %1
+  %3 = zext i32 %2 to i64
+  %4 = load volatile i32, i32* %a
+  %5 = zext i32 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+; Check indexed and unindexed stores
+
+define void @sb(i8 *%a, i8 %b) nounwind {
+; CHECK-LABEL: sb:
+; CHECK: sb a1, 7(a0)
+; CHECK: sb a1, 0(a0)
+  store i8 %b, i8* %a
+  %1 = getelementptr i8, i8* %a, i32 7
+  store i8 %b, i8* %1
+  ret void
+}
+
+define void @sh(i16 *%a, i16 %b) nounwind {
+; CHECK-LABEL: sh:
+; CHECK: sh a1, 16(a0)
+; CHECK: sh a1, 0(a0)
+  store i16 %b, i16* %a
+  %1 = getelementptr i16, i16* %a, i32 8
+  store i16 %b, i16* %1
+  ret void
+}
+
+define void @sw(i32 *%a, i32 %b) nounwind {
+; CHECK-LABEL: sw:
+; CHECK: sw a1, 36(a0)
+; CHECK: sw a1, 0(a0)
+  store i32 %b, i32* %a
+  %1 = getelementptr i32, i32* %a, i32 9
+  store i32 %b, i32* %1
+  ret void
+}
+
+; 64-bit loads and stores
+
+define i64 @ld(i64 *%a) nounwind {
+; CHECK-LABEL: ld:
+; CHECK: ld a1, 0(a0)
+; CHECK: ld a0, 80(a0)
+  %1 = getelementptr i64, i64* %a, i32 10
+  %2 = load i64, i64* %1
+  %3 = load volatile i64, i64* %a
+  ret i64 %2
+}
+
+define void @sd(i64 *%a, i64 %b) nounwind {
+; CHECK-LABEL: sd:
+; CHECK: sd a1, 88(a0)
+; CHECK: sd a1, 0(a0)
+  store i64 %b, i64* %a
+  %1 = getelementptr i64, i64* %a, i32 11
+  store i64 %b, i64* %1
+  ret void
+}
+
+; Check load and store to an i1 location
+define i64 @load_sext_zext_anyext_i1(i1 *%a) nounwind {
+; CHECK-LABEL: load_sext_zext_anyext_i1:
+; CHECK: lb a1, 0(a0)
+; CHECK: lbu a1, 1(a0)
+; CHECK: lbu a0, 2(a0)
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  ; sextload i1
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i64
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i64
+  %7 = add i64 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i64 %7
+}
+
+define i16 @load_sext_zext_anyext_i1_i16(i1 *%a) nounwind {
+; CHECK-LABEL: load_sext_zext_anyext_i1_i16:
+; CHECK: lb a1, 0(a0)
+; CHECK: lbu a1, 1(a0)
+; CHECK: lbu a0, 2(a0)
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  ; sextload i1
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i16
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i16
+  %7 = add i16 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i16 %7
+}
+
+; Check load and store to a global
+@G = global i64 0
+
+define i64 @ld_sd_global(i64 %a) nounwind {
+; TODO: the addi should be folded in to the ld/sd operations
+; CHECK-LABEL: ld_sd_global:
+; CHECK: lui a1, %hi(G)
+; CHECK: addi a2, a1, %lo(G)
+; CHECK: ld a1, 0(a2)
+; CHECK: sd a0, 0(a2)
+; CHECK: lui a2, %hi(G+72)
+; CHECK: addi a2, a2, %lo(G+72)
+; CHECK: ld a3, 0(a2)
+; CHECK: sd a0, 0(a2)
+; CHECK: addi a0, a1, 0
+  %1 = load volatile i64, i64* @G
+  store i64 %a, i64* @G
+  %2 = getelementptr i64, i64* @G, i64 9
+  %3 = load volatile i64, i64* %2
+  store i64 %a, i64* %2
+  ret i64 %1
+}
diff --git a/utils/TableGen/CodeGenDAGPatterns.cpp b/utils/TableGen/CodeGenDAGPatterns.cpp
index 6321c96086e..68453e26d62 100644
--- a/utils/TableGen/CodeGenDAGPatterns.cpp
+++ b/utils/TableGen/CodeGenDAGPatterns.cpp
@@ -517,8 +517,8 @@ bool TypeInfer::EnforceSmallerThan(TypeSetByHwMode &Small,
     if (MinS != S.end()) {
       Changed |= berase_if(B, std::bind(LE, std::placeholders::_1, *MinS));
       if (B.empty()) {
-        TP.error("Type contradiction in " +
-                 Twine(__func__) + ":" + Twine(__LINE__));
+//      TP.error("Type contradiction in " +
+//               Twine(__func__) + ":" + Twine(__LINE__));
         return Changed;
       }
     }
-- 
2.14.2

