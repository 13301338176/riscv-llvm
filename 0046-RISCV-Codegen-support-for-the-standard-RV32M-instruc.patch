From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Codegen support for the standard RV32M instruction set
 extension

---
 lib/Target/RISCV/RISCVISelDAGToDAG.cpp |   2 +-
 lib/Target/RISCV/RISCVISelLowering.cpp |  18 +--
 lib/Target/RISCV/RISCVInstrInfoM.td    |  15 +++
 test/CodeGen/RISCV/div.ll              | 238 ++++++++++++++++++++++++++-------
 test/CodeGen/RISCV/mul.ll              | 186 ++++++++++++++++++++++----
 test/CodeGen/RISCV/rem.ll              |  39 +++++-
 6 files changed, 407 insertions(+), 91 deletions(-)

diff --git a/lib/Target/RISCV/RISCVISelDAGToDAG.cpp b/lib/Target/RISCV/RISCVISelDAGToDAG.cpp
index 39f4560a50c..4119506fc21 100644
--- a/lib/Target/RISCV/RISCVISelDAGToDAG.cpp
+++ b/lib/Target/RISCV/RISCVISelDAGToDAG.cpp
@@ -29,7 +29,7 @@ class RISCVDAGToDAGISel final : public SelectionDAGISel {
   const RISCVSubtarget *Subtarget;
 public:
   explicit RISCVDAGToDAGISel(RISCVTargetMachine &TargetMachine)
-      : SelectionDAGISel(TargetMachine) {}
+      : SelectionDAGISel(TargetMachine), Subtarget(nullptr) {}
 
   StringRef getPassName() const override {
     return "RISCV DAG->DAG Pattern Instruction Selection";
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index bfd5dbbf3da..a988fbc7097 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -77,18 +77,20 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
   setOperationAction(ISD::SUBC, XLenVT, Expand);
   setOperationAction(ISD::SUBE, XLenVT, Expand);
 
-  setOperationAction(ISD::SREM, XLenVT, Expand);
+  if (!Subtarget.hasStdExtM()) {
+    setOperationAction(ISD::MUL, XLenVT, Expand);
+    setOperationAction(ISD::MULHS, XLenVT, Expand);
+    setOperationAction(ISD::MULHU, XLenVT, Expand);
+    setOperationAction(ISD::SDIV, XLenVT, Expand);
+    setOperationAction(ISD::UDIV, XLenVT, Expand);
+    setOperationAction(ISD::SREM, XLenVT, Expand);
+    setOperationAction(ISD::UREM, XLenVT, Expand);
+  }
+
   setOperationAction(ISD::SDIVREM, XLenVT, Expand);
-  setOperationAction(ISD::SDIV, XLenVT, Expand);
-  setOperationAction(ISD::UREM, XLenVT, Expand);
   setOperationAction(ISD::UDIVREM, XLenVT, Expand);
-  setOperationAction(ISD::UDIV, XLenVT, Expand);
-
-  setOperationAction(ISD::MUL, XLenVT, Expand);
   setOperationAction(ISD::SMUL_LOHI, XLenVT, Expand);
   setOperationAction(ISD::UMUL_LOHI, XLenVT, Expand);
-  setOperationAction(ISD::MULHS, XLenVT, Expand);
-  setOperationAction(ISD::MULHU, XLenVT, Expand);
 
   setOperationAction(ISD::SHL_PARTS, XLenVT, Expand);
   setOperationAction(ISD::SRL_PARTS, XLenVT, Expand);
diff --git a/lib/Target/RISCV/RISCVInstrInfoM.td b/lib/Target/RISCV/RISCVInstrInfoM.td
index fec9c1f9399..2dd10ada400 100644
--- a/lib/Target/RISCV/RISCVInstrInfoM.td
+++ b/lib/Target/RISCV/RISCVInstrInfoM.td
@@ -34,3 +34,18 @@ def DIVUW   : ALUW_rr<0b0000001, 0b101, "divuw">;
 def REMW    : ALUW_rr<0b0000001, 0b110, "remw">;
 def REMUW   : ALUW_rr<0b0000001, 0b111, "remuw">;
 } // Predicates = [HasStdExtM, IsRV64]
+
+//===----------------------------------------------------------------------===//
+// Pseudo-instructions and codegen patterns
+//===----------------------------------------------------------------------===//
+
+let Predicates = [HasStdExtM] in {
+def : PatGprGpr<mul, MUL>;
+def : PatGprGpr<mulhs, MULH>;
+def : PatGprGpr<mulhu, MULHU>;
+// No ISDOpcode for mulhsu
+def : PatGprGpr<sdiv, DIV>;
+def : PatGprGpr<udiv, DIVU>;
+def : PatGprGpr<srem, REM>;
+def : PatGprGpr<urem, REMU>;
+} // Predicates = [HasStdExtM]
diff --git a/test/CodeGen/RISCV/div.ll b/test/CodeGen/RISCV/div.ll
index bb0abe2b8db..309710e95a5 100644
--- a/test/CodeGen/RISCV/div.ll
+++ b/test/CodeGen/RISCV/div.ll
@@ -1,97 +1,241 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
+; RUN:   | FileCheck -check-prefix=RV32I %s
+; RUN: llc -mtriple=riscv32 -mattr=+m -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV32IM %s
 
-define i32 @udiv(i32 %a, i32 %b) {
+define i32 @udiv(i32 %a, i32 %b) nounwind {
 ; RV32I-LABEL: udiv:
-; RV32I: lui a2, %hi(__udivsi3)
-; RV32I: addi a2, a2, %lo(__udivsi3)
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__udivsi3)
+; RV32I-NEXT:    addi a2, a2, %lo(__udivsi3)
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: udiv:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    divu a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = udiv i32 %a, %b
   ret i32 %1
 }
 
-define i32 @udiv_constant(i32 %a) {
+define i32 @udiv_constant(i32 %a) nounwind {
 ; RV32I-LABEL: udiv_constant:
-; RV32I: lui a1, %hi(__udivsi3)
-; RV32I: addi a2, a1, %lo(__udivsi3)
-; RV32I: addi a1, zero, 5
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a1, %hi(__udivsi3)
+; RV32I-NEXT:    addi a2, a1, %lo(__udivsi3)
+; RV32I-NEXT:    addi a1, zero, 5
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: udiv_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    lui a1, 838861
+; RV32IM-NEXT:    addi a1, a1, -819
+; RV32IM-NEXT:    mulhu a0, a0, a1
+; RV32IM-NEXT:    srli a0, a0, 2
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = udiv i32 %a, 5
   ret i32 %1
 }
 
-define i32 @udiv_pow2(i32 %a) {
+define i32 @udiv_pow2(i32 %a) nounwind {
 ; RV32I-LABEL: udiv_pow2:
-; RV32I: srli a0, a0, 3
+; RV32I:       # BB#0:
+; RV32I-NEXT:    srli a0, a0, 3
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: udiv_pow2:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    srli a0, a0, 3
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = udiv i32 %a, 8
   ret i32 %1
 }
 
-define i64 @udiv64(i64 %a, i64 %b) {
+define i64 @udiv64(i64 %a, i64 %b) nounwind {
 ; RV32I-LABEL: udiv64:
-; RV32I: lui a4, %hi(__udivdi3)
-; RV32I: addi a4, a4, %lo(__udivdi3)
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a4, %hi(__udivdi3)
+; RV32I-NEXT:    addi a4, a4, %lo(__udivdi3)
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: udiv64:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi sp, sp, -16
+; RV32IM-NEXT:    sw ra, 12(sp)
+; RV32IM-NEXT:    lui a4, %hi(__udivdi3)
+; RV32IM-NEXT:    addi a4, a4, %lo(__udivdi3)
+; RV32IM-NEXT:    jalr ra, a4, 0
+; RV32IM-NEXT:    lw ra, 12(sp)
+; RV32IM-NEXT:    addi sp, sp, 16
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = udiv i64 %a, %b
   ret i64 %1
 }
 
-define i64 @udiv64_constant(i64 %a) {
+define i64 @udiv64_constant(i64 %a) nounwind {
 ; RV32I-LABEL: udiv64_constant:
-; RV32I: lui a2, %hi(__udivdi3)
-; RV32I: addi a4, a2, %lo(__udivdi3)
-; RV32I: addi a2, zero, 5
-; RV32I: addi a3, zero, 0
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__udivdi3)
+; RV32I-NEXT:    addi a4, a2, %lo(__udivdi3)
+; RV32I-NEXT:    addi a2, zero, 5
+; RV32I-NEXT:    addi a3, zero, 0
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: udiv64_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi sp, sp, -16
+; RV32IM-NEXT:    sw ra, 12(sp)
+; RV32IM-NEXT:    lui a2, %hi(__udivdi3)
+; RV32IM-NEXT:    addi a4, a2, %lo(__udivdi3)
+; RV32IM-NEXT:    addi a2, zero, 5
+; RV32IM-NEXT:    addi a3, zero, 0
+; RV32IM-NEXT:    jalr ra, a4, 0
+; RV32IM-NEXT:    lw ra, 12(sp)
+; RV32IM-NEXT:    addi sp, sp, 16
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = udiv i64 %a, 5
   ret i64 %1
 }
 
-define i32 @sdiv(i32 %a, i32 %b) {
+define i32 @sdiv(i32 %a, i32 %b) nounwind {
 ; RV32I-LABEL: sdiv:
-; RV32I: lui a2, %hi(__divsi3)
-; RV32I: addi a2, a2, %lo(__divsi3)
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__divsi3)
+; RV32I-NEXT:    addi a2, a2, %lo(__divsi3)
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: sdiv:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    div a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = sdiv i32 %a, %b
   ret i32 %1
 }
 
-define i32 @sdiv_constant(i32 %a) {
+define i32 @sdiv_constant(i32 %a) nounwind {
 ; RV32I-LABEL: sdiv_constant:
-; RV32I: lui a1, %hi(__divsi3)
-; RV32I: addi a2, a1, %lo(__divsi3)
-; RV32I: addi a1, zero, 5
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a1, %hi(__divsi3)
+; RV32I-NEXT:    addi a2, a1, %lo(__divsi3)
+; RV32I-NEXT:    addi a1, zero, 5
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: sdiv_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    lui a1, 419430
+; RV32IM-NEXT:    addi a1, a1, 1639
+; RV32IM-NEXT:    mulh a0, a0, a1
+; RV32IM-NEXT:    srli a1, a0, 31
+; RV32IM-NEXT:    srai a0, a0, 1
+; RV32IM-NEXT:    add a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = sdiv i32 %a, 5
   ret i32 %1
 }
 
-define i32 @sdiv_pow2(i32 %a) {
-; RV32I-LABEL: sdiv_pow2
-; RV32I: srai a1, a0, 31
-; RV32I: srli a1, a1, 29
-; RV32I: add a0, a0, a1
-; RV32I: srai a0, a0, 3
+define i32 @sdiv_pow2(i32 %a) nounwind {
+; RV32I-LABEL: sdiv_pow2:
+; RV32I:       # BB#0:
+; RV32I-NEXT:    srai a1, a0, 31
+; RV32I-NEXT:    srli a1, a1, 29
+; RV32I-NEXT:    add a0, a0, a1
+; RV32I-NEXT:    srai a0, a0, 3
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: sdiv_pow2:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    srai a1, a0, 31
+; RV32IM-NEXT:    srli a1, a1, 29
+; RV32IM-NEXT:    add a0, a0, a1
+; RV32IM-NEXT:    srai a0, a0, 3
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = sdiv i32 %a, 8
   ret i32 %1
 }
 
-define i64 @sdiv64(i64 %a, i64 %b) {
+define i64 @sdiv64(i64 %a, i64 %b) nounwind {
 ; RV32I-LABEL: sdiv64:
-; RV32I: lui a4, %hi(__divdi3)
-; RV32I: addi a4, a4, %lo(__divdi3)
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a4, %hi(__divdi3)
+; RV32I-NEXT:    addi a4, a4, %lo(__divdi3)
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: sdiv64:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi sp, sp, -16
+; RV32IM-NEXT:    sw ra, 12(sp)
+; RV32IM-NEXT:    lui a4, %hi(__divdi3)
+; RV32IM-NEXT:    addi a4, a4, %lo(__divdi3)
+; RV32IM-NEXT:    jalr ra, a4, 0
+; RV32IM-NEXT:    lw ra, 12(sp)
+; RV32IM-NEXT:    addi sp, sp, 16
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = sdiv i64 %a, %b
   ret i64 %1
 }
 
-define i64 @sdiv64_constant(i64 %a) {
+define i64 @sdiv64_constant(i64 %a) nounwind {
 ; RV32I-LABEL: sdiv64_constant:
-; RV32I: lui a2, %hi(__divdi3)
-; RV32I: addi a4, a2, %lo(__divdi3)
-; RV32I: addi a2, zero, 5
-; RV32I: addi a3, zero, 0
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__divdi3)
+; RV32I-NEXT:    addi a4, a2, %lo(__divdi3)
+; RV32I-NEXT:    addi a2, zero, 5
+; RV32I-NEXT:    addi a3, zero, 0
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: sdiv64_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi sp, sp, -16
+; RV32IM-NEXT:    sw ra, 12(sp)
+; RV32IM-NEXT:    lui a2, %hi(__divdi3)
+; RV32IM-NEXT:    addi a4, a2, %lo(__divdi3)
+; RV32IM-NEXT:    addi a2, zero, 5
+; RV32IM-NEXT:    addi a3, zero, 0
+; RV32IM-NEXT:    jalr ra, a4, 0
+; RV32IM-NEXT:    lw ra, 12(sp)
+; RV32IM-NEXT:    addi sp, sp, 16
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = sdiv i64 %a, 5
   ret i64 %1
 }
diff --git a/test/CodeGen/RISCV/mul.ll b/test/CodeGen/RISCV/mul.ll
index d27f687b046..019f5f8c71e 100644
--- a/test/CodeGen/RISCV/mul.ll
+++ b/test/CodeGen/RISCV/mul.ll
@@ -1,59 +1,189 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
+; RUN:   | FileCheck -check-prefix=RV32I %s
+; RUN: llc -mtriple=riscv32 -mattr=+m -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV32IM %s
 
-define i32 @square(i32 %a) {
+define i32 @square(i32 %a) nounwind {
 ; RV32I-LABEL: square:
-; RV32I: lui a1, %hi(__mulsi3)
-; RV32I: addi a2, a1, %lo(__mulsi3)
-; RV32I: addi a1, a0, 0
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a1, %hi(__mulsi3)
+; RV32I-NEXT:    addi a2, a1, %lo(__mulsi3)
+; RV32I-NEXT:    addi a1, a0, 0
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: square:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    mul a0, a0, a0
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i32 %a, %a
   ret i32 %1
 }
 
-define i32 @mul(i32 %a, i32 %b) {
+define i32 @mul(i32 %a, i32 %b) nounwind {
 ; RV32I-LABEL: mul:
-; RV32I: lui a2, %hi(__mulsi3)
-; RV32I: addi a2, a2, %lo(__mulsi3)
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__mulsi3)
+; RV32I-NEXT:    addi a2, a2, %lo(__mulsi3)
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mul:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    mul a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i32 %a, %b
   ret i32 %1
 }
 
-define i32 @mul_constant(i32 %a) {
+define i32 @mul_constant(i32 %a) nounwind {
 ; RV32I-LABEL: mul_constant:
-; RV32I: lui a1, %hi(__mulsi3)
-; RV32I: addi a2, a1, %lo(__mulsi3)
-; RV32I: addi a1, zero, 5
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a1, %hi(__mulsi3)
+; RV32I-NEXT:    addi a2, a1, %lo(__mulsi3)
+; RV32I-NEXT:    addi a1, zero, 5
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mul_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi a1, zero, 5
+; RV32IM-NEXT:    mul a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i32 %a, 5
   ret i32 %1
 }
 
-define i32 @mul_pow2(i32 %a) {
+define i32 @mul_pow2(i32 %a) nounwind {
 ; RV32I-LABEL: mul_pow2:
-; RV32I: slli a0, a0, 3
-; RV32I: jalr zero, ra, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    slli a0, a0, 3
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mul_pow2:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    slli a0, a0, 3
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i32 %a, 8
   ret i32 %1
 }
 
-define i64 @mul64(i64 %a, i64 %b) {
+define i64 @mul64(i64 %a, i64 %b) nounwind {
 ; RV32I-LABEL: mul64:
-; RV32I: lui a4, %hi(__muldi3)
-; RV32I: addi a4, a4, %lo(__muldi3)
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a4, %hi(__muldi3)
+; RV32I-NEXT:    addi a4, a4, %lo(__muldi3)
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mul64:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    mul a3, a0, a3
+; RV32IM-NEXT:    mulhu a4, a0, a2
+; RV32IM-NEXT:    add a3, a4, a3
+; RV32IM-NEXT:    mul a1, a1, a2
+; RV32IM-NEXT:    add a1, a3, a1
+; RV32IM-NEXT:    mul a0, a0, a2
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i64 %a, %b
   ret i64 %1
 }
 
-define i64 @mul64_constant(i64 %a) {
+define i64 @mul64_constant(i64 %a) nounwind {
 ; RV32I-LABEL: mul64_constant:
-; RV32I: lui a2, %hi(__muldi3)
-; RV32I: addi a4, a2, %lo(__muldi3)
-; RV32I: addi a2, zero, 5
-; RV32I: addi a3, zero, 0
-; RV32I: jalr ra, a4, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__muldi3)
+; RV32I-NEXT:    addi a4, a2, %lo(__muldi3)
+; RV32I-NEXT:    addi a2, zero, 5
+; RV32I-NEXT:    addi a3, zero, 0
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mul64_constant:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    addi a2, zero, 5
+; RV32IM-NEXT:    mul a1, a1, a2
+; RV32IM-NEXT:    mulhu a3, a0, a2
+; RV32IM-NEXT:    add a1, a3, a1
+; RV32IM-NEXT:    mul a0, a0, a2
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = mul i64 %a, 5
   ret i64 %1
 }
+
+define i32 @mulhs(i32 %a, i32 %b) nounwind {
+; RV32I-LABEL: mulhs:
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    addi a2, a1, 0
+; RV32I-NEXT:    lui a1, %hi(__muldi3)
+; RV32I-NEXT:    addi a4, a1, %lo(__muldi3)
+; RV32I-NEXT:    srai a1, a0, 31
+; RV32I-NEXT:    srai a3, a2, 31
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    addi a0, a1, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mulhs:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    mulh a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
+  %1 = sext i32 %a to i64
+  %2 = sext i32 %b to i64
+  %3 = mul i64 %1, %2
+  %4 = lshr i64 %3, 32
+  %5 = trunc i64 %4 to i32
+  ret i32 %5
+}
+
+define i32 @mulhu(i32 %a, i32 %b) nounwind {
+; RV32I-LABEL: mulhu:
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    addi a2, a1, 0
+; RV32I-NEXT:    lui a1, %hi(__muldi3)
+; RV32I-NEXT:    addi a4, a1, %lo(__muldi3)
+; RV32I-NEXT:    addi a1, zero, 0
+; RV32I-NEXT:    addi a3, zero, 0
+; RV32I-NEXT:    jalr ra, a4, 0
+; RV32I-NEXT:    addi a0, a1, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: mulhu:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    mulhu a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
+  %1 = zext i32 %a to i64
+  %2 = zext i32 %b to i64
+  %3 = mul i64 %1, %2
+  %4 = lshr i64 %3, 32
+  %5 = trunc i64 %4 to i32
+  ret i32 %5
+}
diff --git a/test/CodeGen/RISCV/rem.ll b/test/CodeGen/RISCV/rem.ll
index 8a0ed73c37b..95f90221dd2 100644
--- a/test/CodeGen/RISCV/rem.ll
+++ b/test/CodeGen/RISCV/rem.ll
@@ -1,20 +1,45 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
 ; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
-; RUN:   | FileCheck %s -check-prefix=RV32I
+; RUN:   | FileCheck -check-prefix=RV32I %s
+; RUN: llc -mtriple=riscv32 -mattr=+m -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV32IM %s
 
 define i32 @urem(i32 %a, i32 %b) nounwind {
 ; RV32I-LABEL: urem:
-; RV32I: lui a2, %hi(__umodsi3)
-; RV32I: addi a2, a2, %lo(__umodsi3)
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__umodsi3)
+; RV32I-NEXT:    addi a2, a2, %lo(__umodsi3)
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: urem:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    remu a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = urem i32 %a, %b
   ret i32 %1
 }
 
 define i32 @srem(i32 %a, i32 %b) nounwind {
 ; RV32I-LABEL: srem:
-; RV32I: lui a2, %hi(__modsi3)
-; RV32I: addi a2, a2, %lo(__modsi3)
-; RV32I: jalr ra, a2, 0
+; RV32I:       # BB#0:
+; RV32I-NEXT:    addi sp, sp, -16
+; RV32I-NEXT:    sw ra, 12(sp)
+; RV32I-NEXT:    lui a2, %hi(__modsi3)
+; RV32I-NEXT:    addi a2, a2, %lo(__modsi3)
+; RV32I-NEXT:    jalr ra, a2, 0
+; RV32I-NEXT:    lw ra, 12(sp)
+; RV32I-NEXT:    addi sp, sp, 16
+; RV32I-NEXT:    jalr zero, ra, 0
+;
+; RV32IM-LABEL: srem:
+; RV32IM:       # BB#0:
+; RV32IM-NEXT:    rem a0, a0, a1
+; RV32IM-NEXT:    jalr zero, ra, 0
   %1 = srem i32 %a, %b
   ret i32 %1
 }
-- 
2.14.2

