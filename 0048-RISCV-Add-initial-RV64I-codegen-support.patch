From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Add initial RV64I codegen support

Most of RISCVTargetLowering is now parameterised by XLEN.

Note this patch contains a temporary workaround in CodeGenDAGPatterns.cpp.
---
 lib/Target/RISCV/RISCV.td                 |   1 +
 lib/Target/RISCV/RISCVISelLowering.cpp    | 109 ++++++-----
 lib/Target/RISCV/RISCVInstrInfo.cpp       |   7 +-
 lib/Target/RISCV/RISCVInstrInfo.td        |  69 ++++++-
 lib/Target/RISCV/RISCVRegisterInfo.cpp    |  17 +-
 lib/Target/RISCV/RISCVTargetMachine.cpp   |   2 +-
 test/CodeGen/RISCV/addc-adde-sube-subc.ll |  14 ++
 test/CodeGen/RISCV/alu64.ll               | 288 ++++++++++++++++++++++++++++++
 test/CodeGen/RISCV/mem64.ll               | 198 ++++++++++++++++++++
 utils/TableGen/CodeGenDAGPatterns.cpp     |   4 +-
 10 files changed, 632 insertions(+), 77 deletions(-)
 create mode 100644 test/CodeGen/RISCV/alu64.ll
 create mode 100644 test/CodeGen/RISCV/mem64.ll

diff --git a/lib/Target/RISCV/RISCV.td b/lib/Target/RISCV/RISCV.td
index eed9d31b6f9..ce4a491fcc2 100644
--- a/lib/Target/RISCV/RISCV.td
+++ b/lib/Target/RISCV/RISCV.td
@@ -38,6 +38,7 @@ def Feature64Bit   : SubtargetFeature<"64bit", "HasRV64", "true",
                            "Implements RV64">;
 def IsRV64         :  Predicate<"Subtarget->is64Bit()">,
                            AssemblerPredicate<"Feature64Bit">;
+def IsRV32         :  Predicate<"!Subtarget->is64Bit()">;
 
 def RV64           : HwMode<"+64bit">;
 def RV32           : HwMode<"-64bit">;
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index dc85bcdd404..f91c3e83d10 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -40,11 +40,10 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
                                          const RISCVSubtarget &STI)
     : TargetLowering(TM), Subtarget(STI) {
 
+  MVT XLenVT = Subtarget.getXLenVT();
+
   // Set up the register classes.
-  if (Subtarget.is64Bit())
-    addRegisterClass(MVT::i64, &RISCV::GPRRegClass);
-  else
-    addRegisterClass(MVT::i32, &RISCV::GPRRegClass);
+  addRegisterClass(XLenVT, &RISCV::GPRRegClass);
 
   // Compute derived properties from the register classes.
   computeRegisterProperties(STI.getRegisterInfo());
@@ -52,15 +51,15 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
   setStackPointerRegisterToSaveRestore(RISCV::X2);
 
   for (auto N : {ISD::EXTLOAD, ISD::SEXTLOAD, ISD::ZEXTLOAD})
-    setLoadExtAction(N, MVT::i32, MVT::i1, Promote);
+    setLoadExtAction(N, XLenVT, MVT::i1, Promote);
 
   // TODO: add all necessary setOperationAction calls.
-  setOperationAction(ISD::DYNAMIC_STACKALLOC, MVT::i32, Expand);
+  setOperationAction(ISD::DYNAMIC_STACKALLOC, XLenVT, Expand);
 
   setOperationAction(ISD::BR_JT, MVT::Other, Expand);
-  setOperationAction(ISD::BR_CC, MVT::i32, Expand);
-  setOperationAction(ISD::SELECT_CC, MVT::i32, Custom);
-  setOperationAction(ISD::SELECT, MVT::i32, Expand);
+  setOperationAction(ISD::BR_CC, XLenVT, Expand);
+  setOperationAction(ISD::SELECT_CC, XLenVT, Custom);
+  setOperationAction(ISD::SELECT, XLenVT, Expand);
 
   setOperationAction(ISD::STACKSAVE, MVT::Other, Expand);
   setOperationAction(ISD::STACKRESTORE, MVT::Other, Expand);
@@ -73,39 +72,39 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
   for (auto VT : {MVT::i1, MVT::i8, MVT::i16})
     setOperationAction(ISD::SIGN_EXTEND_INREG, VT, Expand);
 
-  setOperationAction(ISD::ADDC, MVT::i32, Expand);
-  setOperationAction(ISD::ADDE, MVT::i32, Expand);
-  setOperationAction(ISD::SUBC, MVT::i32, Expand);
-  setOperationAction(ISD::SUBE, MVT::i32, Expand);
+  setOperationAction(ISD::ADDC, XLenVT, Expand);
+  setOperationAction(ISD::ADDE, XLenVT, Expand);
+  setOperationAction(ISD::SUBC, XLenVT, Expand);
+  setOperationAction(ISD::SUBE, XLenVT, Expand);
 
   if (!Subtarget.hasStdExtM()) {
-    setOperationAction(ISD::MUL, MVT::i32, Expand);
-    setOperationAction(ISD::MULHS, MVT::i32, Expand);
-    setOperationAction(ISD::MULHU, MVT::i32, Expand);
-    setOperationAction(ISD::SDIV, MVT::i32, Expand);
-    setOperationAction(ISD::UDIV, MVT::i32, Expand);
-    setOperationAction(ISD::SREM, MVT::i32, Expand);
-    setOperationAction(ISD::UREM, MVT::i32, Expand);
-  }
-
-  setOperationAction(ISD::SDIVREM, MVT::i32, Expand);
-  setOperationAction(ISD::UDIVREM, MVT::i32, Expand);
-  setOperationAction(ISD::SMUL_LOHI, MVT::i32, Expand);
-  setOperationAction(ISD::UMUL_LOHI, MVT::i32, Expand);
-
-  setOperationAction(ISD::SHL_PARTS, MVT::i32, Expand);
-  setOperationAction(ISD::SRL_PARTS, MVT::i32, Expand);
-  setOperationAction(ISD::SRA_PARTS, MVT::i32, Expand);
-
-  setOperationAction(ISD::ROTL, MVT::i32, Expand);
-  setOperationAction(ISD::ROTR, MVT::i32, Expand);
-  setOperationAction(ISD::BSWAP, MVT::i32, Expand);
-  setOperationAction(ISD::CTTZ, MVT::i32, Expand);
-  setOperationAction(ISD::CTLZ, MVT::i32, Expand);
-  setOperationAction(ISD::CTPOP, MVT::i32, Expand);
-
-  setOperationAction(ISD::GlobalAddress, MVT::i32, Custom);
-  setOperationAction(ISD::BlockAddress, MVT::i32, Custom);
+    setOperationAction(ISD::MUL, XLenVT, Expand);
+    setOperationAction(ISD::MULHS, XLenVT, Expand);
+    setOperationAction(ISD::MULHU, XLenVT, Expand);
+    setOperationAction(ISD::SDIV, XLenVT, Expand);
+    setOperationAction(ISD::UDIV, XLenVT, Expand);
+    setOperationAction(ISD::SREM, XLenVT, Expand);
+    setOperationAction(ISD::UREM, XLenVT, Expand);
+  }
+
+  setOperationAction(ISD::SDIVREM, XLenVT, Expand);
+  setOperationAction(ISD::UDIVREM, XLenVT, Expand);
+  setOperationAction(ISD::SMUL_LOHI, XLenVT, Expand);
+  setOperationAction(ISD::UMUL_LOHI, XLenVT, Expand);
+
+  setOperationAction(ISD::SHL_PARTS, XLenVT, Expand);
+  setOperationAction(ISD::SRL_PARTS, XLenVT, Expand);
+  setOperationAction(ISD::SRA_PARTS, XLenVT, Expand);
+
+  setOperationAction(ISD::ROTL, XLenVT, Expand);
+  setOperationAction(ISD::ROTR, XLenVT, Expand);
+  setOperationAction(ISD::BSWAP, XLenVT, Expand);
+  setOperationAction(ISD::CTTZ, XLenVT, Expand);
+  setOperationAction(ISD::CTLZ, XLenVT, Expand);
+  setOperationAction(ISD::CTPOP, XLenVT, Expand);
+
+  setOperationAction(ISD::GlobalAddress, XLenVT, Custom);
+  setOperationAction(ISD::BlockAddress, XLenVT, Custom);
 
   setBooleanContents(ZeroOrOneBooleanContent);
 
@@ -184,7 +183,7 @@ SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,
   const GlobalValue *GV = N->getGlobal();
   int64_t Offset = N->getOffset();
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue GAHi =
         DAG.getTargetGlobalAddress(GV, DL, Ty, Offset, RISCVII::MO_HI);
     SDValue GALo =
@@ -206,7 +205,7 @@ SDValue RISCVTargetLowering::lowerBlockAddress(SDValue Op,
   const BlockAddress *BA = N->getBlockAddress();
   int64_t Offset = N->getOffset();
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue BAHi = DAG.getTargetBlockAddress(BA, Ty, Offset, RISCVII::MO_HI);
     SDValue BALo = DAG.getTargetBlockAddress(BA, Ty, Offset, RISCVII::MO_LO);
     SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, Ty, BAHi), 0);
@@ -227,7 +226,7 @@ SDValue RISCVTargetLowering::lowerExternalSymbol(SDValue Op,
 
   // TODO: should also handle gp-relative loads
 
-  if (!isPositionIndependent() && !Subtarget.is64Bit()) {
+  if (!isPositionIndependent()) {
     SDValue GAHi = DAG.getTargetExternalSymbol(Sym, Ty, RISCVII::MO_HI);
     SDValue GALo = DAG.getTargetExternalSymbol(Sym, Ty, RISCVII::MO_LO);
     SDValue MNHi = SDValue(DAG.getMachineNode(RISCV::LUI, DL, Ty, GAHi), 0);
@@ -250,7 +249,7 @@ SDValue RISCVTargetLowering::lowerSELECT_CC(SDValue Op,
 
   normaliseSetCC(LHS, RHS, CC);
 
-  SDValue TargetCC = DAG.getConstant(CC, DL, MVT::i32);
+  SDValue TargetCC = DAG.getConstant(CC, DL, Subtarget.getXLenVT());
 
   SDVTList VTs = DAG.getVTList(Op.getValueType(), MVT::Glue);
   SDValue Ops[] = {LHS, RHS, TargetCC, TrueV, FalseV};
@@ -280,14 +279,14 @@ SDValue RISCVTargetLowering::LowerFRAMEADDR(SDValue Op,
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MFI.setFrameAddressIsTaken(true);
   unsigned FrameReg = RI.getFrameRegister(MF);
+  int XLenInBytes = Subtarget.getXLen()/8;
 
   EVT VT = Op.getValueType();
   SDLoc DL(Op);
   SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), DL, FrameReg, VT);
   unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue();
   while (Depth--) {
-    // TODO: adjust for RV64
-    const unsigned Offset = -8;
+    int Offset = -(XLenInBytes*2);
     SDValue Ptr = DAG.getNode(ISD::ADD, DL, VT, FrameAddr,
                               DAG.getIntPtrConstant(Offset, DL));
     FrameAddr =
@@ -302,6 +301,8 @@ SDValue RISCVTargetLowering::LowerRETURNADDR(SDValue Op,
   MachineFunction &MF = DAG.getMachineFunction();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MFI.setReturnAddressIsTaken(true);
+  MVT XLenVT = Subtarget.getXLenVT();
+  int XLenInBytes = Subtarget.getXLen()/8;
 
   if (verifyReturnAddressArgumentIsConstant(Op, DAG))
     return SDValue();
@@ -310,9 +311,9 @@ SDValue RISCVTargetLowering::LowerRETURNADDR(SDValue Op,
   SDLoc DL(Op);
   unsigned Depth = cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue();
   if (Depth) {
+    int Off = -XLenInBytes;
     SDValue FrameAddr = LowerFRAMEADDR(Op, DAG);
-    // TODO: adjust for RV64
-    SDValue Offset = DAG.getConstant(-4, DL, MVT::i32);
+    SDValue Offset = DAG.getConstant(Off, DL, VT);
     return DAG.getLoad(VT, DL, DAG.getEntryNode(),
                        DAG.getNode(ISD::ADD, DL, VT, FrameAddr, Offset),
                        MachinePointerInfo());
@@ -320,8 +321,8 @@ SDValue RISCVTargetLowering::LowerRETURNADDR(SDValue Op,
 
   // Return the value of the return address register, marking it an implicit
   // live-in.
-  unsigned Reg = MF.addLiveIn(RI.getRARegister(), getRegClassFor(MVT::i32));
-  return DAG.getCopyFromReg(DAG.getEntryNode(), DL, Reg, VT);
+  unsigned Reg = MF.addLiveIn(RI.getRARegister(), getRegClassFor(XLenVT));
+  return DAG.getCopyFromReg(DAG.getEntryNode(), DL, Reg, XLenVT);
 }
 
 MachineBasicBlock *
@@ -611,9 +612,8 @@ SDValue RISCVTargetLowering::LowerFormalArguments(
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MachineRegisterInfo &RegInfo = MF.getRegInfo();
   EVT PtrVT = getPointerTy(DAG.getDataLayout());
-  unsigned XLen = DAG.getDataLayout().getLargestLegalIntTypeSizeInBits();
-  unsigned XLenInBytes = XLen/8;
-  MVT XLenVT = XLen == 32 ? MVT::i32 : MVT::i64;
+  MVT XLenVT = Subtarget.getXLenVT();
+  unsigned XLenInBytes = Subtarget.getXLen()/8;
   // Used with vargs to acumulate store chains.
   std::vector<SDValue> OutChains;
 
@@ -753,8 +753,7 @@ SDValue RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
   CallingConv::ID CallConv = CLI.CallConv;
   bool IsVarArg = CLI.IsVarArg;
   EVT PtrVT = getPointerTy(DAG.getDataLayout());
-  unsigned XLen = DAG.getDataLayout().getLargestLegalIntTypeSizeInBits();
-  MVT XLenVT = XLen == 32 ? MVT::i32 : MVT::i64;
+  MVT XLenVT = Subtarget.getXLenVT();
 
   MachineFunction &MF = DAG.getMachineFunction();
 
diff --git a/lib/Target/RISCV/RISCVInstrInfo.cpp b/lib/Target/RISCV/RISCVInstrInfo.cpp
index 13babf7c710..7c1e9fddff5 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.cpp
+++ b/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -57,7 +57,7 @@ void RISCVInstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
     DL = I->getDebugLoc();
 
   if (RC == &RISCV::GPRRegClass)
-    BuildMI(MBB, I, DL, get(RISCV::SW_FI))
+    BuildMI(MBB, I, DL, get(RISCV::StXLEN_FI))
         .addReg(SrcReg, getKillRegState(IsKill))
         .addFrameIndex(FI)
         .addImm(0);
@@ -75,7 +75,7 @@ void RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
     DL = I->getDebugLoc();
 
   if (RC == &RISCV::GPRRegClass)
-    BuildMI(MBB, I, DL, get(RISCV::LW_FI), DestReg).addFrameIndex(FI).addImm(0);
+    BuildMI(MBB, I, DL, get(RISCV::LdXLEN_FI), DestReg).addFrameIndex(FI).addImm(0);
   else
     llvm_unreachable("Can't load this register from stack slot");
 }
@@ -289,9 +289,8 @@ unsigned RISCVInstrInfo::insertIndirectBranch(MachineBasicBlock &MBB,
   MachineFunction *MF = MBB.getParent();
   MachineRegisterInfo &MRI = MF->getRegInfo();
   const auto &TM = static_cast<const RISCVTargetMachine &>(MF->getTarget());
-  const auto &STI = MF->getSubtarget<RISCVSubtarget>();
 
-  if (TM.isPositionIndependent() || STI.is64Bit())
+  if (TM.isPositionIndependent())
     report_fatal_error("Unable to insert indirect branch");
 
   if (!isInt<32>(BrOffset))
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
index c73c1763bc0..764f875edb2 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.td
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -17,7 +17,7 @@ def SDT_RISCVCallSeqStart : SDCallSeqStart<[SDTCisVT<0, i32>,
                                             SDTCisVT<1, i32>]>;
 def SDT_RISCVCallSeqEnd   : SDCallSeqEnd<[SDTCisVT<0, i32>,
                                          SDTCisVT<1, i32>]>;
-def SDT_RISCVCall         : SDTypeProfile<0, -1, [SDTCisVT<0, i32>]>;
+def SDT_RISCVCall         : SDTypeProfile<0, -1, [SDTCisVT<0, XLenVT>]>;
 def SDT_RISCVSelectCC     : SDTypeProfile<1, 5, [SDTCisSameAs<1, 2>,
                                                  SDTCisSameAs<0, 4>,
                                                  SDTCisSameAs<4, 5>]>;
@@ -195,6 +195,7 @@ class SHIFT32_ri<bit arithshift, bits<3> funct3, string OpcodeStr> :
       FI32Shift<arithshift, funct3, 0b0010011, (outs GPR:$rd), (ins GPR:$rs1, uimm5:$shamt),
          OpcodeStr#"\t$rd, $rs1, $shamt", []>;
 
+// TODO: accept and encode a 6-bit shift amount for RV64
 def SLLI : SHIFT32_ri<0, 0b001, "slli">;
 def SRLI : SHIFT32_ri<0, 0b101, "srli">;
 def SRAI : SHIFT32_ri<1, 0b101, "srai">;
@@ -316,8 +317,10 @@ class PatGprUimm5<SDPatternOperator OpNode, FI32Shift Inst> :
 
 /// Immediates
 
+def simm32 : ImmLeaf<XLenVT, [{return isInt<32>(Imm);}]>;
+
 def : Pat<(simm12:$imm), (ADDI X0, simm12:$imm)>;
-def : Pat<(XLenVT imm:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+def : Pat<(simm32:$imm), (ADDI (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>, Requires<[IsRV32]>;
 
 /// Simple arithmetic operations
 
@@ -361,9 +364,9 @@ def : Pat<(setle GPR:$rs1, GPR:$rs2), (XORI (SLT GPR:$rs2, GPR:$rs1), 1)>;
 
 let usesCustomInserter = 1 in
 def Select : Pseudo<(outs GPR:$dst),
-                    (ins GPR:$lhs, GPR:$rhs, i32imm:$imm, GPR:$src, GPR:$src2),
-                    [(set i32:$dst,
-                     (SelectCC GPR:$lhs, GPR:$rhs, (i32 imm:$imm), GPR:$src, GPR:$src2))]>;
+                    (ins GPR:$lhs, GPR:$rhs, ixlenimm:$imm, GPR:$src, GPR:$src2),
+                    [(set XLenVT:$dst,
+                     (SelectCC GPR:$lhs, GPR:$rhs, (XLenVT imm:$imm), GPR:$src, GPR:$src2))]>;
 
 /// Branches and jumps
 
@@ -429,11 +432,11 @@ defm : LdPat<sextloadi8, LB>;
 defm : LdPat<extloadi8, LB>;
 defm : LdPat<sextloadi16, LH>;
 defm : LdPat<extloadi16, LH>;
-defm : LdPat<load, LW>;
+defm : LdPat<load, LW>, Requires<[IsRV32]>;
 defm : LdPat<zextloadi8, LBU>;
 defm : LdPat<zextloadi16, LHU>;
 
-def LW_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr),
+def LdXLEN_FI : Pseudo<(outs GPR:$dst), (ins MEMii:$addr),
                    [(set GPR:$dst, (load ADDRii:$addr))]>;
 
 
@@ -447,9 +450,9 @@ multiclass StPat<PatFrag StoreOp, RISCVInst Inst> {
 
 defm : StPat<truncstorei8, SB>;
 defm : StPat<truncstorei16, SH>;
-defm : StPat<store, SW>;
+defm : StPat<store, SW>, Requires<[IsRV32]>;
 
-def SW_FI : Pseudo<(outs), (ins GPR:$src, MEMii:$addr),
+def StXLEN_FI : Pseudo<(outs), (ins GPR:$src, MEMii:$addr),
                    [(store GPR:$src, ADDRii:$addr)]>;
 
 /// Other pseudo-instructions
@@ -462,6 +465,54 @@ let Defs = [X2], Uses = [X2] in {
                                 [(CallSeqEnd timm:$amt1, timm:$amt2)]>;
 }
 
+/// RV64 patterns
+
+
+// Extract least significant 12 bits from the upper half of an immediate value
+// and sign extend them.
+def HLO12Sext : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(SignExtend64<12>(Val),
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
+// Extract the most significant 20 bits from the upper half of an immediate
+// value. Add 1 if bit 32+11 is 1, to compensate for the low 12 bits in the
+// matching immediate addi or ld/st being negative.
+def HHI20 : SDNodeXForm<imm, [{
+  uint64_t Val = N->getZExtValue() >> 32;
+  return CurDAG->getTargetConstant(((Val+0x800) >> 12) & 0xfffff,
+                                   SDLoc(N), N->getValueType(0));
+}]>;
+
+def simm64 : ImmLeaf<XLenVT, [{return isInt<64>(Imm);}]>;
+
+let Predicates = [IsRV64] in {
+def : Pat<(simm32:$imm), (ADDIW (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm))>;
+// TODO: add better patterns for loading 64-bit immediates
+def : Pat<(simm64:$imm), (OR (SLLI (ADDIW (LUI (HHI20 imm:$imm)), (HLO12Sext imm:$imm)), (i64 32)),
+                             (SRLI (SLLI (ADDIW (LUI (HI20 imm:$imm)), (LO12Sext imm:$imm)), 32), 32))>;
+
+def : Pat<(sext_inreg (add GPR:$rs1, simm12:$imm12), i32), (ADDIW GPR:$rs1, simm12:$imm12)>;
+// sext.w
+def : Pat<(sext_inreg GPR:$rs1, i32), (ADDIW GPR:$rs1, 0)>;
+def : Pat<(sext_inreg (shl GPR:$rs1, uimm5:$shamt), i32), (SLLIW GPR:$rs1, uimm5:$shamt)>;
+// TODO: Patterns for SRLIW, SRAIW
+def : Pat<(sext_inreg (add GPR:$rs1, GPR:$rs2), i32), (ADDW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (sub GPR:$rs1, GPR:$rs2), i32), (SUBW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sext_inreg (shl GPR:$rs1, GPR:$rs2), i32), (SLLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(srl (and GPR:$rs1, 0xffffffff), GPR:$rs2), (SRLW GPR:$rs1, GPR:$rs2)>;
+def : Pat<(sra (sext_inreg GPR:$rs1, i32), GPR:$rs2), (SRAW GPR:$rs1, GPR:$rs2)>;
+
+defm : LdPat<sextloadi32, LW>;
+defm : LdPat<extloadi32, LW>;
+defm : LdPat<zextloadi32, LWU>;
+defm : LdPat<load, LD>;
+
+defm : StPat<truncstorei32, SW>;
+defm : StPat<store, SD>;
+}
+
 //===----------------------------------------------------------------------===//
 // Standard extensions
 
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.cpp b/lib/Target/RISCV/RISCVRegisterInfo.cpp
index 6d05adc6e9f..a81f0fc9514 100644
--- a/lib/Target/RISCV/RISCVRegisterInfo.cpp
+++ b/lib/Target/RISCV/RISCVRegisterInfo.cpp
@@ -61,6 +61,7 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   MachineInstr &MI = *II;
   MachineFunction &MF = *MI.getParent()->getParent();
+  const auto &Subtarget = MF.getSubtarget<RISCVSubtarget>();
   RISCVMachineFunctionInfo *RVFI = MF.getInfo<RISCVMachineFunctionInfo>();
   MachineFrameInfo &MFI = MF.getFrameInfo();
   MachineRegisterInfo &MRI = MF.getRegInfo();
@@ -124,14 +125,20 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
     FrameRegFlags = RegState::Kill;
   }
 
+  unsigned Opc;
+
   switch (MI.getOpcode()) {
-  case RISCV::LW_FI:
-    BuildMI(MBB, II, DL, TII->get(RISCV::LW), Reg)
+  default:
+    llvm_unreachable("Unexpected opcode");
+  case RISCV::LdXLEN_FI:
+    Opc = Subtarget.is64Bit() ? RISCV::LD : RISCV::LW;
+    BuildMI(MBB, II, DL, TII->get(Opc), Reg)
         .addReg(FrameReg, FrameRegFlags)
         .addImm(Offset);
     break;
-  case RISCV::SW_FI:
-    BuildMI(MBB, II, DL, TII->get(RISCV::SW))
+  case RISCV::StXLEN_FI:
+    Opc = Subtarget.is64Bit() ? RISCV::SD : RISCV::SW;
+    BuildMI(MBB, II, DL, TII->get(Opc))
         .addReg(Reg, getKillRegState(MI.getOperand(0).isKill()))
         .addReg(FrameReg, FrameRegFlags | RegState::Kill)
         .addImm(Offset);
@@ -141,8 +148,6 @@ void RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
         .addReg(FrameReg, FrameRegFlags)
         .addImm(Offset);
     break;
-  default:
-    llvm_unreachable("Unexpected opcode");
   }
 
   // Erase old instruction.
diff --git a/lib/Target/RISCV/RISCVTargetMachine.cpp b/lib/Target/RISCV/RISCVTargetMachine.cpp
index 9b7fa590f91..e75fb3b701c 100644
--- a/lib/Target/RISCV/RISCVTargetMachine.cpp
+++ b/lib/Target/RISCV/RISCVTargetMachine.cpp
@@ -30,7 +30,7 @@ extern "C" void LLVMInitializeRISCVTarget() {
 
 static std::string computeDataLayout(const Triple &TT) {
   if (TT.isArch64Bit()) {
-    return "e-m:e-i64:64-n32:64-S128";
+    return "e-m:e-p:64:64-i64:64-i128:128-n64-S128";
   } else {
     assert(TT.isArch32Bit() && "only RV32 and RV64 are currently supported");
     return "e-m:e-p:32:32-i64:64-n32-S128";
diff --git a/test/CodeGen/RISCV/addc-adde-sube-subc.ll b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
index def5b76a79c..04d58c808e0 100644
--- a/test/CodeGen/RISCV/addc-adde-sube-subc.ll
+++ b/test/CodeGen/RISCV/addc-adde-sube-subc.ll
@@ -24,3 +24,17 @@ define i64 @subc_sube(i64 %a, i64 %b) {
   %1 = sub i64 %a, %b
   ret i64 %1
 }
+
+define i128 @addc_adde128(i128 %a, i128 %b) {
+; CHECK-LABEL: addc_adde128:
+; TODO
+  %1 = add i128 %a, %b
+  ret i128 %1
+}
+
+define i128 @subc_sube128(i128 %a, i128 %b) {
+; CHECK-LABEL: subc_sube128:
+; TODO
+  %1 = sub i128 %a, %b
+  ret i128 %1
+}
diff --git a/test/CodeGen/RISCV/alu64.ll b/test/CodeGen/RISCV/alu64.ll
new file mode 100644
index 00000000000..07c405d5804
--- /dev/null
+++ b/test/CodeGen/RISCV/alu64.ll
@@ -0,0 +1,288 @@
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s | FileCheck %s
+
+; These tests are each targeted at a particular RISC-V RV64I ALU instruction. 
+; Other files in this folder exercise LLVM IR instructions that don't directly 
+; match RISC-V instruction
+
+; Register-immediate instructions
+
+define i64 @addi(i64 %a) nounwind {
+; CHECK-LABEL: addi:
+; CHECK: addi a0, a0, 1
+; CHECK: jalr zero, ra, 0
+; TODO: check support for materialising larger constants
+  %1 = add i64 %a, 1
+  ret i64 %1
+}
+
+define i64 @slti(i64 %a) nounwind {
+; CHECK-LABEL: slti:
+; CHECK: slti a0, a0, 2
+; CHECK: jalr zero, ra, 0
+  %1 = icmp slt i64 %a, 2
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltiu(i64 %a) nounwind {
+; CHECK-LABEL: sltiu:
+; CHECK: sltiu a0, a0, 3
+; CHECK: jalr zero, ra, 0
+  %1 = icmp ult i64 %a, 3
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xori(i64 %a) nounwind {
+; CHECK-LABEL: xori:
+; CHECK: xori a0, a0, 4
+; CHECK: jalr zero, ra, 0
+  %1 = xor i64 %a, 4
+  ret i64 %1
+}
+
+define i64 @ori(i64 %a) nounwind {
+; CHECK-LABEL: ori:
+; CHECK: ori a0, a0, 5
+; CHECK: jalr zero, ra, 0
+  %1 = or i64 %a, 5
+  ret i64 %1
+}
+
+define i64 @andi(i64 %a) nounwind {
+; CHECK-LABEL: andi:
+; CHECK: andi a0, a0, 6
+; CHECK: jalr zero, ra, 0
+  %1 = and i64 %a, 6
+  ret i64 %1
+}
+
+define i64 @slli(i64 %a) nounwind {
+; CHECK-LABEL: slli:
+; CHECK: slli a0, a0, 7
+; CHECK: jalr zero, ra, 0
+  %1 = shl i64 %a, 7
+  ret i64 %1
+}
+
+define i64 @srli(i64 %a) nounwind {
+; CHECK-LABEL: srli:
+; CHECK: srli a0, a0, 8
+; CHECK: jalr zero, ra, 0
+  %1 = lshr i64 %a, 8
+  ret i64 %1
+}
+
+define i64 @srai(i64 %a) nounwind {
+; CHECK-LABEL: srai:
+; CHECK: srai a0, a0, 9
+; CHECK: jalr zero, ra, 0
+  %1 = ashr i64 %a, 9
+  ret i64 %1
+}
+
+; Register-register instructions
+
+define i64 @add(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: add:
+; CHECK: add a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = add i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sub(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sub:
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = sub i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sll(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sll:
+; CHECK: sll a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = shl i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @slt(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: slt:
+; CHECK: slt a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = icmp slt i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @sltu(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sltu:
+; CHECK: sltu a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = icmp ult i64 %a, %b
+  %2 = zext i1 %1 to i64
+  ret i64 %2
+}
+
+define i64 @xor(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: xor:
+; CHECK: xor a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = xor i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @srl(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: srl:
+; CHECK: srl a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = lshr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @sra(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: sra:
+; CHECK: sra a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = ashr i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @or(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: or:
+; CHECK: or a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = or i64 %a, %b
+  ret i64 %1
+}
+
+define i64 @and(i64 %a, i64 %b) nounwind {
+; CHECK-LABEL: and:
+; CHECK: and a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  %1 = and i64 %a, %b
+  ret i64 %1
+}
+
+define signext i32 @addiw(i32 signext %a) {
+; CHECK-LABEL: addiw:
+; CHECK: addiw a0, a0, 123
+; CHECK: jalr zero, ra, 0
+  %1 = add i32 %a, 123
+  ret i32 %1
+}
+
+define signext i32 @slliw(i32 signext %a) {
+; CHECK-LABEL: slliw:
+; CHECK: slliw a0, a0, 17
+; CHECK: jalr zero, ra, 0
+  %1 = shl i32 %a, 17
+  ret i32 %1
+}
+
+define signext i32 @srliw(i32 signext %a) {
+; CHECK-LABEL: srliw:
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sraiw(i32 signext %a) {
+; CHECK-LABEL: sraiw:
+; TODO
+  ret i32 %a
+}
+
+define signext i32 @sextw(i32 zeroext %a) {
+; CHECK-LABEL: sextw:
+; CHECK: addiw a0, a0, 0
+  ret i32 %a
+}
+
+define signext i32 @addw(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: addw:
+; CHECK: addw a0, a0, a1
+  %1 = add i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @subw(i32 signext %a, i32 signext %b) {
+; CHECK-LABEL: subw:
+; CHECK: subw a0, a0, a1
+  %1 = sub i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sllw(i32 signext %a, i32 zeroext %b) {
+; CHECK-LABEL: sllw:
+; CHECK: sllw a0, a0, a1
+  %1 = shl i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @srlw(i32 signext %a, i32 zeroext %b) {
+; CHECK-LABEL: srlw:
+; CHECK: srlw a0, a0, a1
+  %1 = lshr i32 %a, %b
+  ret i32 %1
+}
+
+define signext i32 @sraw(i64 %a, i32 zeroext %b) {
+; CHECK-LABEL: sraw:
+; CHECK: sraw a0, a0, a1
+  %1 = trunc i64 %a to i32
+  %2 = ashr i32 %1, %b
+  ret i32 %2
+}
+
+; Materialize constants
+
+define i64 @zero() {
+; CHECK-LABEL: zero:
+; CHECK: addi a0, zero, 0
+; CHECK: jalr zero, ra, 0
+  ret i64 0
+}
+
+define i64 @pos_small() {
+; CHECK-LABEL: pos_small:
+; CHECK: addi a0, zero, 2047
+; CHECK: jalr zero, ra, 0
+  ret i64 2047
+}
+
+define i64 @neg_small() {
+; CHECK-LABEL: neg_small:
+; CHECK: addi a0, zero, -2048
+; CHECK: jalr zero, ra, 0
+  ret i64 -2048
+}
+
+define i64 @pos_imm32() {
+; CHECK-LABEL: pos_imm32:
+; CHECK: lui [[REG:[a-z0-9]+]], 423811
+; CHECK: addiw a0, [[REG]], -1297
+; CHECK: jalr zero, ra, 0
+  ret i64 1735928559
+}
+
+define i64 @neg_imm32() {
+; CHECK-LABEL: neg_imm32:
+; CHECK: lui [[REG:[a-z0-9]+]], 912092
+; CHECK: addiw a0, [[REG]], -273
+; CHECK: jalr zero, ra, 0
+  ret i64 -559038737
+}
+
+define i64 @pos_imm64() {
+; CHECK-LABEL: pos_imm64:
+; CHECK: lui a0, 7018
+; CHECK: addiw a0, a0, -1212
+; CHECK: slli  a0, a0, 32
+; CHECK: lui a1, 430355
+; CHECK: addiw a1, a1, 787
+; CHECK: slli  a1, a1, 32
+; CHECK: srli  a1, a1, 32
+; CHECK: or  a0, a0, a1
+  ret i64 123456757922083603
+}
diff --git a/test/CodeGen/RISCV/mem64.ll b/test/CodeGen/RISCV/mem64.ll
new file mode 100644
index 00000000000..2560b988ed3
--- /dev/null
+++ b/test/CodeGen/RISCV/mem64.ll
@@ -0,0 +1,198 @@
+; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s | FileCheck %s
+
+; Check indexed and unindexed, sext, zext and anyext loads
+
+define i64 @lb(i8 *%a) nounwind {
+; CHECK-LABEL: lb:
+; CHECK: lb a1, 0(a0)
+; CHECK: lb a0, 1(a0)
+  %1 = getelementptr i8, i8* %a, i32 1
+  %2 = load i8, i8* %1
+  %3 = sext i8 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i8, i8* %a
+  ret i64 %3
+}
+
+define i64 @lh(i16 *%a) nounwind {
+; CHECK-LABEL: lh:
+; CHECK: lh a1, 0(a0)
+; CHECK: lh a0, 4(a0)
+  %1 = getelementptr i16, i16* %a, i32 2
+  %2 = load i16, i16* %1
+  %3 = sext i16 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i16, i16* %a
+  ret i64 %3
+}
+
+define i64 @lw(i32 *%a) nounwind {
+; CHECK-LABEL: lw:
+; CHECK: lw a1, 0(a0)
+; CHECK: lw a0, 12(a0)
+  %1 = getelementptr i32, i32* %a, i32 3
+  %2 = load i32, i32* %1
+  %3 = sext i32 %2 to i64
+  ; the unused load will produce an anyext for selection
+  %4 = load volatile i32, i32* %a
+  ret i64 %3
+}
+
+define i64 @lbu(i8 *%a) nounwind {
+; CHECK-LABEL: lbu:
+; CHECK: lbu a1, 0(a0)
+; CHECK: lbu a0, 4(a0)
+  %1 = getelementptr i8, i8* %a, i32 4
+  %2 = load i8, i8* %1
+  %3 = zext i8 %2 to i64
+  %4 = load volatile i8, i8* %a
+  %5 = zext i8 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lhu(i16 *%a) nounwind {
+; CHECK-LABEL: lhu:
+; CHECK: lhu a1, 0(a0)
+; CHECK: lhu a0, 10(a0)
+  %1 = getelementptr i16, i16* %a, i32 5
+  %2 = load i16, i16* %1
+  %3 = zext i16 %2 to i64
+  %4 = load volatile i16, i16* %a
+  %5 = zext i16 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+define i64 @lwu(i32 *%a) nounwind {
+; CHECK-LABEL: lwu:
+; CHECK: lwu a1, 0(a0)
+; CHECK: lwu a0, 24(a0)
+  %1 = getelementptr i32, i32* %a, i32 6
+  %2 = load i32, i32* %1
+  %3 = zext i32 %2 to i64
+  %4 = load volatile i32, i32* %a
+  %5 = zext i32 %4 to i64
+  %6 = add i64 %3, %5
+  ret i64 %6
+}
+
+; Check indexed and unindexed stores
+
+define void @sb(i8 *%a, i8 %b) nounwind {
+; CHECK-LABEL: sb:
+; CHECK: sb a1, 7(a0)
+; CHECK: sb a1, 0(a0)
+  store i8 %b, i8* %a
+  %1 = getelementptr i8, i8* %a, i32 7
+  store i8 %b, i8* %1
+  ret void
+}
+
+define void @sh(i16 *%a, i16 %b) nounwind {
+; CHECK-LABEL: sh:
+; CHECK: sh a1, 16(a0)
+; CHECK: sh a1, 0(a0)
+  store i16 %b, i16* %a
+  %1 = getelementptr i16, i16* %a, i32 8
+  store i16 %b, i16* %1
+  ret void
+}
+
+define void @sw(i32 *%a, i32 %b) nounwind {
+; CHECK-LABEL: sw:
+; CHECK: sw a1, 36(a0)
+; CHECK: sw a1, 0(a0)
+  store i32 %b, i32* %a
+  %1 = getelementptr i32, i32* %a, i32 9
+  store i32 %b, i32* %1
+  ret void
+}
+
+; 64-bit loads and stores
+
+define i64 @ld(i64 *%a) nounwind {
+; CHECK-LABEL: ld:
+; CHECK: ld a1, 0(a0)
+; CHECK: ld a0, 80(a0)
+  %1 = getelementptr i64, i64* %a, i32 10
+  %2 = load i64, i64* %1
+  %3 = load volatile i64, i64* %a
+  ret i64 %2
+}
+
+define void @sd(i64 *%a, i64 %b) nounwind {
+; CHECK-LABEL: sd:
+; CHECK: sd a1, 88(a0)
+; CHECK: sd a1, 0(a0)
+  store i64 %b, i64* %a
+  %1 = getelementptr i64, i64* %a, i32 11
+  store i64 %b, i64* %1
+  ret void
+}
+
+; Check load and store to an i1 location
+define i64 @load_sext_zext_anyext_i1(i1 *%a) nounwind {
+; CHECK-LABEL: load_sext_zext_anyext_i1:
+; CHECK: lb a1, 0(a0)
+; CHECK: lbu a1, 1(a0)
+; CHECK: lbu a0, 2(a0)
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  ; sextload i1
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i64
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i64
+  %7 = add i64 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i64 %7
+}
+
+define i16 @load_sext_zext_anyext_i1_i16(i1 *%a) nounwind {
+; CHECK-LABEL: load_sext_zext_anyext_i1_i16:
+; CHECK: lb a1, 0(a0)
+; CHECK: lbu a1, 1(a0)
+; CHECK: lbu a0, 2(a0)
+; CHECK: sub a0, a0, a1
+; CHECK: jalr zero, ra, 0
+  ; sextload i1
+  %1 = getelementptr i1, i1* %a, i32 1
+  %2 = load i1, i1* %1
+  %3 = sext i1 %2 to i16
+  ; zextload i1
+  %4 = getelementptr i1, i1* %a, i32 2
+  %5 = load i1, i1* %4
+  %6 = zext i1 %5 to i16
+  %7 = add i16 %3, %6
+  ; extload i1 (anyext). Produced as the load is unused.
+  %8 = load volatile i1, i1* %a
+  ret i16 %7
+}
+
+; Check load and store to a global
+@G = global i64 0
+
+define i64 @ld_sd_global(i64 %a) nounwind {
+; TODO: the addi should be folded in to the ld/sd operations
+; CHECK-LABEL: ld_sd_global:
+; CHECK: lui a1, %hi(G)
+; CHECK: addi a2, a1, %lo(G)
+; CHECK: ld a1, 0(a2)
+; CHECK: sd a0, 0(a2)
+; CHECK: lui a2, %hi(G+72)
+; CHECK: addi a2, a2, %lo(G+72)
+; CHECK: ld a3, 0(a2)
+; CHECK: sd a0, 0(a2)
+; CHECK: addi a0, a1, 0
+  %1 = load volatile i64, i64* @G
+  store i64 %a, i64* @G
+  %2 = getelementptr i64, i64* @G, i64 9
+  %3 = load volatile i64, i64* %2
+  store i64 %a, i64* %2
+  ret i64 %1
+}
diff --git a/utils/TableGen/CodeGenDAGPatterns.cpp b/utils/TableGen/CodeGenDAGPatterns.cpp
index 6321c96086e..68453e26d62 100644
--- a/utils/TableGen/CodeGenDAGPatterns.cpp
+++ b/utils/TableGen/CodeGenDAGPatterns.cpp
@@ -517,8 +517,8 @@ bool TypeInfer::EnforceSmallerThan(TypeSetByHwMode &Small,
     if (MinS != S.end()) {
       Changed |= berase_if(B, std::bind(LE, std::placeholders::_1, *MinS));
       if (B.empty()) {
-        TP.error("Type contradiction in " +
-                 Twine(__func__) + ":" + Twine(__LINE__));
+//      TP.error("Type contradiction in " +
+//               Twine(__func__) + ":" + Twine(__LINE__));
         return Changed;
       }
     }
-- 
2.14.1

