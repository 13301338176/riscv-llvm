From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Alex Bradbury <asb@lowrisc.org>
Subject: [RISCV] Codegen support for RV32D floating point load/store

Also includes support for spilling and restoring an FPR64 to the stack, and
tests for loading f64 immediates from the constant pool. double-imm.ll
required setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand);
---
 lib/Target/RISCV/RISCVISelLowering.cpp           |   2 +
 lib/Target/RISCV/RISCVInstrInfo.cpp              |   4 +
 lib/Target/RISCV/RISCVInstrInfoD.td              |   6 ++
 test/CodeGen/RISCV/double-imm.ll                 |  34 +++++++
 test/CodeGen/RISCV/double-mem.ll                 | 122 +++++++++++++++++++++++
 test/CodeGen/RISCV/double-stack-spill-restore.ll |  52 ++++++++++
 6 files changed, 220 insertions(+)
 create mode 100644 test/CodeGen/RISCV/double-imm.ll
 create mode 100644 test/CodeGen/RISCV/double-stack-spill-restore.ll

diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
index 7b835fe7d71..c11f86f8696 100644
--- a/lib/Target/RISCV/RISCVISelLowering.cpp
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -131,6 +131,8 @@ RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &TM,
     setOperationAction(ISD::SELECT_CC, MVT::f64, Expand);
     setOperationAction(ISD::SELECT, MVT::f64, Custom);
     setOperationAction(ISD::BR_CC, MVT::f64, Expand);
+    setLoadExtAction(ISD::EXTLOAD, MVT::f64, MVT::f32, Expand);
+    setTruncStoreAction(MVT::f64, MVT::f32, Expand);
   }
 
   setOperationAction(ISD::GlobalAddress, XLenVT, Custom);
diff --git a/lib/Target/RISCV/RISCVInstrInfo.cpp b/lib/Target/RISCV/RISCVInstrInfo.cpp
index 00bdbd96ea2..28cd4af2a86 100644
--- a/lib/Target/RISCV/RISCVInstrInfo.cpp
+++ b/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -72,6 +72,8 @@ void RISCVInstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
     Opcode = RISCV::StXLEN_FI;
   else if (RC == &RISCV::FPR32RegClass)
     Opcode = RISCV::StFPR32_FI;
+  else if (RC == &RISCV::FPR64RegClass)
+    Opcode = RISCV::StFPR64_FI;
   else
     llvm_unreachable("Can't store this register to stack slot");
 
@@ -96,6 +98,8 @@ void RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
     Opcode = RISCV::LdXLEN_FI;
   else if (RC == &RISCV::FPR32RegClass)
     Opcode = RISCV::LdFPR32_FI;
+  else if (RC == &RISCV::FPR64RegClass)
+    Opcode = RISCV::LdFPR64_FI;
   else
     llvm_unreachable("Can't load this register from stack slot");
 
diff --git a/lib/Target/RISCV/RISCVInstrInfoD.td b/lib/Target/RISCV/RISCVInstrInfoD.td
index 2dd9236d449..5e1022757bd 100644
--- a/lib/Target/RISCV/RISCVInstrInfoD.td
+++ b/lib/Target/RISCV/RISCVInstrInfoD.td
@@ -232,11 +232,17 @@ def Select_FPR64_Using_CC_GPR : SelectCC_rrirr<FPR64, GPR>;
 
 /// Loads
 
+defm : LdPat<load, FLD>;
+
 def LdFPR64_FI : Pseudo<(outs FPR64:$dst), (ins MEMii:$addr),
                         [(set FPR64:$dst, (load ADDRii:$addr))]>;
 
 /// Stores
 
+def : Pat<(store FPR64:$rs2, GPR:$rs1), (FSD FPR64:$rs2, GPR:$rs1, 0)>;
+def : Pat<(store FPR64:$rs2, (add GPR:$rs1, simm12:$imm12)),
+            (FSD FPR64:$rs2, GPR:$rs1, simm12:$imm12)>;
+
 def StFPR64_FI : Pseudo<(outs), (ins FPR64:$src, MEMii:$addr),
                         [(store FPR64:$src, ADDRii:$addr)]>;
 
diff --git a/test/CodeGen/RISCV/double-imm.ll b/test/CodeGen/RISCV/double-imm.ll
new file mode 100644
index 00000000000..28df1c163db
--- /dev/null
+++ b/test/CodeGen/RISCV/double-imm.ll
@@ -0,0 +1,34 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -mtriple=riscv32 -mattr=+d -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV32IFD %s
+
+define double @double_imm() nounwind {
+; RV32IFD-LABEL: double_imm:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    lui a0, 345155
+; RV32IFD-NEXT:    addi a0, a0, -744
+; RV32IFD-NEXT:    lui a1, 262290
+; RV32IFD-NEXT:    addi a1, a1, 507
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  ret double 3.1415926535897931159979634685441851615905761718750
+}
+
+define double @double_imm_op(double %a) nounwind {
+; RV32IFD-LABEL: double_imm_op:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -16
+; RV32IFD-NEXT:    sw a1, 12(sp)
+; RV32IFD-NEXT:    sw a0, 8(sp)
+; RV32IFD-NEXT:    lui a0, %hi(.LCPI1_0)
+; RV32IFD-NEXT:    addi a0, a0, %lo(.LCPI1_0)
+; RV32IFD-NEXT:    fld ft0, 0(a0)
+; RV32IFD-NEXT:    fld ft1, 8(sp)
+; RV32IFD-NEXT:    fadd.d ft0, ft1, ft0
+; RV32IFD-NEXT:    fsd ft0, 0(sp)
+; RV32IFD-NEXT:    lw a0, 0(sp)
+; RV32IFD-NEXT:    lw a1, 4(sp)
+; RV32IFD-NEXT:    addi sp, sp, 16
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = fadd double %a, 1.0
+  ret double %1
+}
diff --git a/test/CodeGen/RISCV/double-mem.ll b/test/CodeGen/RISCV/double-mem.ll
index 28a3790ef7c..8be46a44f95 100644
--- a/test/CodeGen/RISCV/double-mem.ll
+++ b/test/CodeGen/RISCV/double-mem.ll
@@ -2,6 +2,111 @@
 ; RUN: llc -mtriple=riscv32 -mattr=+d -verify-machineinstrs < %s \
 ; RUN:   | FileCheck -check-prefix=RV32IFD %s
 
+define double @fld(double *%a) nounwind {
+; RV32IFD-LABEL: fld:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -16
+; RV32IFD-NEXT:    fld ft0, 24(a0)
+; RV32IFD-NEXT:    fld ft1, 0(a0)
+; RV32IFD-NEXT:    fadd.d ft0, ft1, ft0
+; RV32IFD-NEXT:    fsd ft0, 8(sp)
+; RV32IFD-NEXT:    lw a0, 8(sp)
+; RV32IFD-NEXT:    lw a1, 12(sp)
+; RV32IFD-NEXT:    addi sp, sp, 16
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = load double, double* %a
+  %2 = getelementptr double, double* %a, i32 3
+  %3 = load double, double* %2
+; Use both loaded values in an FP op to ensure an fld is used, even for the
+; soft double ABI
+  %4 = fadd double %1, %3
+  ret double %4
+}
+
+define void @fsd(double *%a, double %b, double %c) nounwind {
+; Use %b and %c in an FP op to ensure doubleing point registers are used, even
+; for the soft double ABI
+; RV32IFD-LABEL: fsd:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -16
+; RV32IFD-NEXT:    sw a4, 4(sp)
+; RV32IFD-NEXT:    sw a3, 0(sp)
+; RV32IFD-NEXT:    sw a2, 12(sp)
+; RV32IFD-NEXT:    sw a1, 8(sp)
+; RV32IFD-NEXT:    fld ft0, 0(sp)
+; RV32IFD-NEXT:    fld ft1, 8(sp)
+; RV32IFD-NEXT:    fadd.d ft0, ft1, ft0
+; RV32IFD-NEXT:    fsd ft0, 64(a0)
+; RV32IFD-NEXT:    fsd ft0, 0(a0)
+; RV32IFD-NEXT:    addi sp, sp, 16
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = fadd double %b, %c
+  store double %1, double* %a
+  %2 = getelementptr double, double* %a, i32 8
+  store double %1, double* %2
+  ret void
+}
+
+; Check load and store to a global
+@G = global double 0.0
+
+define double @fld_fsd_global(double %a, double %b) nounwind {
+; Use %a and %b in an FP op to ensure doubleing point registers are used, even
+; for the soft double ABI
+; RV32IFD-LABEL: fld_fsd_global:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -32
+; RV32IFD-NEXT:    sw a3, 20(sp)
+; RV32IFD-NEXT:    sw a2, 16(sp)
+; RV32IFD-NEXT:    sw a1, 28(sp)
+; RV32IFD-NEXT:    sw a0, 24(sp)
+; RV32IFD-NEXT:    fld ft0, 16(sp)
+; RV32IFD-NEXT:    fld ft1, 24(sp)
+; RV32IFD-NEXT:    fadd.d ft0, ft1, ft0
+; RV32IFD-NEXT:    lui a0, %hi(G)
+; RV32IFD-NEXT:    fld ft1, %lo(G)(a0)
+; RV32IFD-NEXT:    fsd ft0, %lo(G)(a0)
+; RV32IFD-NEXT:    lui a0, %hi(G+72)
+; RV32IFD-NEXT:    fld ft1, %lo(G+72)(a0)
+; RV32IFD-NEXT:    fsd ft0, %lo(G+72)(a0)
+; RV32IFD-NEXT:    fsd ft0, 8(sp)
+; RV32IFD-NEXT:    lw a0, 8(sp)
+; RV32IFD-NEXT:    lw a1, 12(sp)
+; RV32IFD-NEXT:    addi sp, sp, 32
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = fadd double %a, %b
+  %2 = load volatile double, double* @G
+  store double %1, double* @G
+  %3 = getelementptr double, double* @G, i32 9
+  %4 = load volatile double, double* %3
+  store double %1, double* %3
+  ret double %1
+}
+
+; Ensure that 1 is added to the high 20 bits if bit 11 of the low part is 1
+define double @fld_fsd_constant(double %a) nounwind {
+; RV32IFD-LABEL: fld_fsd_constant:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -16
+; RV32IFD-NEXT:    sw a1, 12(sp)
+; RV32IFD-NEXT:    sw a0, 8(sp)
+; RV32IFD-NEXT:    lui a0, 912092
+; RV32IFD-NEXT:    fld ft0, -273(a0)
+; RV32IFD-NEXT:    fld ft1, 8(sp)
+; RV32IFD-NEXT:    fadd.d ft0, ft1, ft0
+; RV32IFD-NEXT:    fsd ft0, -273(a0)
+; RV32IFD-NEXT:    fsd ft0, 0(sp)
+; RV32IFD-NEXT:    lw a0, 0(sp)
+; RV32IFD-NEXT:    lw a1, 4(sp)
+; RV32IFD-NEXT:    addi sp, sp, 16
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = inttoptr i32 3735928559 to double*
+  %2 = load volatile double, double* %1
+  %3 = fadd double %a, %2
+  store double %3, double* %1
+  ret double %3
+}
+
 declare void @notdead(i8*)
 
 define double @fld_stack(double %a) nounwind {
@@ -61,3 +166,20 @@ define void @fsd_stack(double %a, double %b) nounwind {
   call void @notdead(i8* %3)
   ret void
 }
+
+; Test selection of store<ST4[%a], trunc to f32>, ..
+define void @fsd_trunc(float* %a, double %b) nounwind noinline optnone {
+; RV32IFD-LABEL: fsd_trunc:
+; RV32IFD:       # BB#0:
+; RV32IFD-NEXT:    addi sp, sp, -16
+; RV32IFD-NEXT:    sw a2, 12(sp)
+; RV32IFD-NEXT:    sw a1, 8(sp)
+; RV32IFD-NEXT:    fld ft0, 8(sp)
+; RV32IFD-NEXT:    fcvt.s.d ft0, ft0
+; RV32IFD-NEXT:    fsw ft0, 0(a0)
+; RV32IFD-NEXT:    addi sp, sp, 16
+; RV32IFD-NEXT:    jalr zero, ra, 0
+  %1 = fptrunc double %b to float
+  store float %1, float* %a, align 4
+  ret void
+}
diff --git a/test/CodeGen/RISCV/double-stack-spill-restore.ll b/test/CodeGen/RISCV/double-stack-spill-restore.ll
new file mode 100644
index 00000000000..d1dccaa357f
--- /dev/null
+++ b/test/CodeGen/RISCV/double-stack-spill-restore.ll
@@ -0,0 +1,52 @@
+; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
+; RUN: llc -mtriple=riscv32 -mattr=+d -verify-machineinstrs < %s \
+; RUN:   | FileCheck -check-prefix=RV32IFD %s
+
+define double @func(double %d, i32 %n) nounwind {
+; RV32IFD-LABEL: func:
+; RV32IFD:       # BB#0: # %entry
+; RV32IFD-NEXT:    addi sp, sp, -64
+; RV32IFD-NEXT:    sw ra, 60(sp)
+; RV32IFD-NEXT:    sw a1, 52(sp)
+; RV32IFD-NEXT:    sw a0, 48(sp)
+; RV32IFD-NEXT:    fld ft0, 48(sp)
+; RV32IFD-NEXT:    beq a2, zero, .LBB0_2
+; RV32IFD-NEXT:  # BB#1: # %if.else
+; RV32IFD-NEXT:    fsd ft0, 40(sp)
+; RV32IFD-NEXT:    addi a2, a2, -1
+; RV32IFD-NEXT:    lui a0, %hi(func)
+; RV32IFD-NEXT:    addi a3, a0, %lo(func)
+; RV32IFD-NEXT:    lw a0, 40(sp)
+; RV32IFD-NEXT:    lw a1, 44(sp)
+; RV32IFD-NEXT:    fsd ft0, 8(sp)
+; RV32IFD-NEXT:    jalr ra, a3, 0
+; RV32IFD-NEXT:    sw a1, 36(sp)
+; RV32IFD-NEXT:    sw a0, 32(sp)
+; RV32IFD-NEXT:    fld ft0, 32(sp)
+; RV32IFD-NEXT:    fld ft1, 8(sp)
+; RV32IFD-NEXT:    fadd.d ft0, ft0, ft1
+; RV32IFD-NEXT:    fsd ft0, 24(sp)
+; RV32IFD-NEXT:    lw a0, 24(sp)
+; RV32IFD-NEXT:    lw a1, 28(sp)
+; RV32IFD-NEXT:    jal zero, .LBB0_3
+; RV32IFD-NEXT:  .LBB0_2: # %return
+; RV32IFD-NEXT:    fsd ft0, 16(sp)
+; RV32IFD-NEXT:    lw a0, 16(sp)
+; RV32IFD-NEXT:    lw a1, 20(sp)
+; RV32IFD-NEXT:  .LBB0_3: # %return
+; RV32IFD-NEXT:    lw ra, 60(sp)
+; RV32IFD-NEXT:    addi sp, sp, 64
+; RV32IFD-NEXT:    jalr zero, ra, 0
+entry:
+  %cmp = icmp eq i32 %n, 0
+  br i1 %cmp, label %return, label %if.else
+
+if.else:
+  %sub = add i32 %n, -1
+  %call = tail call double @func(double %d, i32 %sub)
+  %add = fadd double %call, %d
+  ret double %add
+
+return:
+  ret double %d
+}
-- 
2.15.0

